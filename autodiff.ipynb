{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EricLBuehler/Automatic-Differentiation-Custom/blob/main/autodiff.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PZkYrhKktTc"
      },
      "source": [
        "#Automatic Differentiation\n",
        "\n",
        "Automatic Differentiation is a core tool used to calculate derivatives which are key to machine learning. This is my personal implementation.\n",
        "\n",
        "Eric Buehler 2023"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-CAtru2sknMQ"
      },
      "outputs": [],
      "source": [
        "# AD\n",
        "#https://towardsdatascience.com/build-your-own-automatic-differentiation-program-6ecd585eec2a\n",
        "#https://e-dorigatti.github.io/math/deep%20learning/2020/04/07/autodiff.html\n",
        "#https://jingnanshi.com/blog/autodiff.html\n",
        "#https://github.com/karpathy/micrograd\n",
        "#https://sidsite.com/posts/autodiff/\n",
        "\n",
        "# Simple NN implementation\n",
        "#https://mostafa-samir.github.io/auto-diff-pt2/#putting-everything-into-action\n",
        "#https://stackoverflow.com/questions/67615051/implementing-binary-cross-entropy-loss-gives-different-answer-than-tensorflows\n",
        "\n",
        "# Softmax derivative\n",
        "#https://aimatters.wordpress.com/2019/06/17/the-softmax-function-derivative/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "T_UIIcR4pT-T"
      },
      "outputs": [],
      "source": [
        "from abc import ABC, abstractmethod\n",
        "from typing import *\n",
        "from functools import reduce\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a base abstract class for all values, and a general OperatorLike class."
      ],
      "metadata": {
        "id": "geGY6CR4UcaM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-z244F_fpke4"
      },
      "outputs": [],
      "source": [
        "class DifferentiableValue(ABC):\n",
        "    count = 0\n",
        "    def __init__(self):\n",
        "        DifferentiableValue.count += 1\n",
        "        self.id = DifferentiableValue.count \n",
        "\n",
        "    @abstractmethod\n",
        "    def backward(self, var):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def forward(self):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def __repr__(self) -> str:\n",
        "        pass\n",
        "\n",
        "class OperatorLike(ABC):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "General functions\n",
        "\n",
        "- `generate_topo` topographically sorts a graph, ensuring a directed acyclic graph."
      ],
      "metadata": {
        "id": "5OQJUq07UtJ9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "CJOjSKB2Qi0O"
      },
      "outputs": [],
      "source": [
        "def generate_topo(graph: DifferentiableValue) -> List[DifferentiableValue]:\n",
        "    topo = []\n",
        "    visited = set()\n",
        "    def build_topo(node: DifferentiableValue) -> List[DifferentiableValue]:\n",
        "        if node not in visited:\n",
        "            visited.add(node)\n",
        "            if hasattr(node, \"inputs\"):\n",
        "                for input in node.inputs:\n",
        "                    build_topo(input)\n",
        "            topo.append(node)\n",
        "        return topo\n",
        "    return build_topo(graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a graph to hold the operations, and a gradient graph to hold gradients."
      ],
      "metadata": {
        "id": "oIb72qpYUnJh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "NBTQOAp-rzN4"
      },
      "outputs": [],
      "source": [
        "class GradientGraph:\n",
        "    def __init__(self, graph: List[DifferentiableValue]):\n",
        "        self.graph = graph\n",
        "    \n",
        "    def wrt(self, var: Union[DifferentiableValue, np.ndarray]) -> np.ndarray:\n",
        "        if isinstance(var,  np.ndarray):\n",
        "            array = []\n",
        "            for item in np.nditer(var.flatten(),[\"refs_ok\"]):\n",
        "                if item not in self.graph:\n",
        "                    raise KeyError(f\"Variable {var} not found in gradient graph.\")\n",
        "                \n",
        "                for v in self.graph:\n",
        "                    if v == item:\n",
        "                        array.append(v)\n",
        "            return np.array(array)\n",
        "\n",
        "\n",
        "        if var not in self.graph:\n",
        "            raise KeyError(f\"Variable {var} not found in gradient graph.\")\n",
        "        for v in self.graph:\n",
        "            if v == var:\n",
        "                return v.gradient\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"GradientGraph: {}\".format(\", \".join([str(item) for item in self.graph]))\n",
        "\n",
        "class Graph:\n",
        "    def __init__(self):\n",
        "        self.values = []\n",
        "        global _graph\n",
        "        _graph = self\n",
        "        self.has_backwarded = False\n",
        "        self.clean_graph = lambda values: (values := values[:-1])\n",
        "\n",
        "    def __enter__(self):\n",
        "        return self\n",
        "    \n",
        "    def __exit__(self, exc_type, exc_value, traceback):\n",
        "        pass\n",
        "\n",
        "    def forward(self) -> Union[np.ndarray, SupportsFloat]:\n",
        "        self.clean_graph(self.values)\n",
        "        return self.values[-1].forward()\n",
        "\n",
        "    def backward(self) -> np.ndarray:\n",
        "        self.clean_graph(self.values)\n",
        "        graph = self.values.copy()\n",
        "        res = self.values[-1].backward()\n",
        "        self.values = graph\n",
        "        self.has_backwarded = True\n",
        "        return res\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"Graph: {}\".format(\", \".join([str(item) for item in self.values]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_gradient_graph(item) -> GradientGraph:\n",
        "    filtered = set()\n",
        "    topo = generate_topo(item)\n",
        "    for value in topo:\n",
        "        if not (isinstance(value, OperatorLike) and isinstance(value, Variable)):\n",
        "            filtered.add(value)\n",
        "            \n",
        "    return GradientGraph(list(filtered))"
      ],
      "metadata": {
        "id": "duCL-WEZKZEJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define constant and variable values.\n",
        "\n",
        "The `.backward` functions return the derivative of constants and variables."
      ],
      "metadata": {
        "id": "U0ZoBayKWJLf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "jPRsl5tqqcQ4"
      },
      "outputs": [],
      "source": [
        "class Constant(DifferentiableValue):\n",
        "    count = 0\n",
        "\n",
        "    def __init__(self, value: Union[SupportsFloat, np.ndarray]):\n",
        "        super().__init__()\n",
        "        self.value = value\n",
        "        Constant.count += 1\n",
        "        \n",
        "        self.gradient = 0\n",
        "        \n",
        "    def backward(self):\n",
        "        self.gradient = 0\n",
        "                \n",
        "        return _graph\n",
        "        \n",
        "    def forward(self) -> Any:\n",
        "        return self.value\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return f\"Constant({self.value}, g={self.gradient})\"\n",
        "\n",
        "class Variable(DifferentiableValue):\n",
        "    count = 0\n",
        "\n",
        "    def __init__(self, value: Union[SupportsFloat, np.ndarray] = None, name = None):\n",
        "        super().__init__()\n",
        "        self._value = value\n",
        "        self.name = name\n",
        "        Variable.count += 1\n",
        "        \n",
        "        self.gradient = 0\n",
        "        \n",
        "    def backward(self):\n",
        "        self.gradient = 1\n",
        "                \n",
        "        return self\n",
        "    \n",
        "    @property\n",
        "    def value(self):\n",
        "        return self._value\n",
        "    \n",
        "    @value.setter\n",
        "    def value(self, value):\n",
        "        if self.value == None:\n",
        "            raise ValueError(\"Variable does not have value\")\n",
        "        self._value = value\n",
        "        \n",
        "    def forward(self) -> Union[np.ndarray, SupportsFloat]:\n",
        "        if self.value == None:\n",
        "            raise ValueError(\"Variable does not have value\")\n",
        "        return self.value\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return f\"Variable('{self.name}' {self.value}, g={self.gradient})\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define operation \"nodes\" that act like values.\n",
        "\n",
        "The `.backward` functions return the effective value after applying the chain rule, for sums, products, and powers.\n",
        "\n",
        "Note that the `self.gradient = 1` essentially means that the gradient of y with respect to y is 1 (because `self` is the first element of `reversed(topo`).\n",
        "\n",
        "See https://jingnanshi.com/blog/autodiff.html for a great explanation.\n",
        "Note that the equations above table 1 show how the gradients are accumulated, like can be seen in the `_backward` functions."
      ],
      "metadata": {
        "id": "-JBZxTj5WrZ_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "lSeVTXmRx3Nb"
      },
      "outputs": [],
      "source": [
        "class Sum(DifferentiableValue, OperatorLike):\n",
        "    count = 0\n",
        "\n",
        "    def __init__(self, left: DifferentiableValue, right:  DifferentiableValue):\n",
        "        super().__init__()\n",
        "        _graph.values.append(self)\n",
        "        self.inputs = [left, right]\n",
        "        Sum.count += 1\n",
        "        \n",
        "        self.gradient = 0\n",
        "\n",
        "        def _backward():\n",
        "            for input in self.inputs:\n",
        "                input.gradient += self.gradient\n",
        "\n",
        "        self._backward = _backward\n",
        "        \n",
        "    def backward(self):\n",
        "        topo = generate_topo(self)\n",
        "        \n",
        "        self.gradient = 1\n",
        "        for node in reversed(topo):\n",
        "            if hasattr(node, \"_backward\"):\n",
        "                node._backward()\n",
        "                \n",
        "        return self\n",
        "        \n",
        "    def forward(self) -> Union[np.ndarray, SupportsFloat]:\n",
        "        return sum([input.forward() for input in self.inputs])\n",
        "    \n",
        "    def __repr__(self) -> str:\n",
        "        return \"Sum({}, g={})\".format(', '.join([str(input) for input in self.inputs]), self.gradient)\n",
        "\n",
        "class Product(DifferentiableValue, OperatorLike):\n",
        "    count = 0\n",
        "\n",
        "    def __init__(self, left: DifferentiableValue, right:  DifferentiableValue):\n",
        "        super().__init__()\n",
        "        _graph.values.append(self)\n",
        "        self.inputs = [left, right]\n",
        "        Product.count += 1\n",
        "        \n",
        "        self.gradient = 0\n",
        "\n",
        "        def _backward():\n",
        "            self.inputs[0].gradient += self.inputs[1].forward() * self.gradient\n",
        "            self.inputs[1].gradient += self.inputs[0].forward() * self.gradient\n",
        "\n",
        "        self._backward = _backward\n",
        "        \n",
        "    def backward(self):\n",
        "        topo = generate_topo(self)\n",
        "        \n",
        "        self.gradient = 1\n",
        "        for node in reversed(topo):\n",
        "            if hasattr(node, \"_backward\"):\n",
        "                node._backward()\n",
        "                \n",
        "        return self\n",
        "        \n",
        "    def forward(self) -> Union[np.ndarray, SupportsFloat]:\n",
        "        return reduce((lambda x, y: x * y), [input.forward() for input in self.inputs])\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return \"Product({}, g={})\".format(', '.join([str(input) for input in self.inputs]), self.gradient)\n",
        "\n",
        "class Power(DifferentiableValue, OperatorLike):\n",
        "    count = 0\n",
        "\n",
        "    def __init__(self, base: DifferentiableValue, pow:  DifferentiableValue):\n",
        "        super().__init__()\n",
        "        _graph.values.append(self)\n",
        "        self.inputs = [base, pow]\n",
        "        Power.count += 1\n",
        "        \n",
        "        self.gradient = 0\n",
        "\n",
        "        def _backward():\n",
        "            self.inputs[0].gradient += (self.inputs[1].forward() * self.inputs[0].forward() ** (self.inputs[1] - 1).forward()) * self.gradient\n",
        "            self.inputs[1].gradient += abs(np.log(self.inputs[0].forward())) * self.inputs[0].forward() ** (self.inputs[1].forward()) * self.gradient\n",
        "\n",
        "        self._backward = _backward\n",
        "        \n",
        "    def backward(self):\n",
        "        topo = generate_topo(self)\n",
        "        \n",
        "        self.gradient = 1\n",
        "        for node in reversed(topo):\n",
        "            if hasattr(node, \"_backward\"):\n",
        "                node._backward()\n",
        "                \n",
        "        return self\n",
        "        \n",
        "    def forward(self) -> Union[np.ndarray, SupportsFloat]:\n",
        "        return reduce((lambda x, y: x ** y), [input.forward() for input in self.inputs])\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return \"Power({}, g={})\".format(', '.join([str(input) for input in self.inputs]), self.gradient)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trigonometric and other functions"
      ],
      "metadata": {
        "id": "4won-zWcCLf9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Sine(DifferentiableValue, OperatorLike):\n",
        "    count = 0\n",
        "\n",
        "    def __init__(self, x: DifferentiableValue):\n",
        "        super().__init__()\n",
        "        _graph.values.append(self)\n",
        "        self.inputs = [x]\n",
        "        Sine.count += 1\n",
        "        \n",
        "        self.gradient = 0\n",
        "\n",
        "        def _backward():\n",
        "            self.inputs[0].gradient += np.cos(self.inputs[0].forward()) * self.gradient\n",
        "\n",
        "        self._backward = _backward\n",
        "        \n",
        "    def backward(self):\n",
        "        topo = generate_topo(self)\n",
        "        \n",
        "        self.gradient = 1\n",
        "        for node in reversed(topo):\n",
        "            if hasattr(node, \"_backward\"):\n",
        "                node._backward()\n",
        "                \n",
        "        return self\n",
        "        \n",
        "    def forward(self) -> Union[np.ndarray, SupportsFloat]:\n",
        "        return np.sin(self.inputs[0].forward())\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return \"Sine({}, g={})\".format(', '.join([str(input) for input in self.inputs]), self.gradient)\n",
        "\n",
        "class Cosine(DifferentiableValue, OperatorLike):\n",
        "    count = 0\n",
        "\n",
        "    def __init__(self, x: DifferentiableValue):\n",
        "        super().__init__()\n",
        "        _graph.values.append(self)\n",
        "        self.inputs = [x]\n",
        "        Cosine.count += 1\n",
        "        \n",
        "        self.gradient = 0\n",
        "\n",
        "        def _backward():\n",
        "            self.inputs[0].gradient += -np.sin(self.inputs[0].forward()) * self.gradient\n",
        "\n",
        "        self._backward = _backward\n",
        "        \n",
        "    def backward(self):\n",
        "        topo = generate_topo(self)\n",
        "        \n",
        "        self.gradient = 1\n",
        "        for node in reversed(topo):\n",
        "            if hasattr(node, \"_backward\"):\n",
        "                node._backward()\n",
        "                \n",
        "        return self\n",
        "        \n",
        "    def forward(self) -> Union[np.ndarray, SupportsFloat]:\n",
        "        return np.cos(self.inputs[0].forward())\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return \"Cosine({}, g={})\".format(', '.join([str(input) for input in self.inputs]), self.gradient)\n",
        "\n",
        "class Tangent(DifferentiableValue, OperatorLike):\n",
        "    count = 0\n",
        "\n",
        "    def __init__(self, x: DifferentiableValue):\n",
        "        super().__init__()\n",
        "        _graph.values.append(self)\n",
        "        self.inputs = [x]\n",
        "        Tangent.count += 1\n",
        "        \n",
        "        self.gradient = 0\n",
        "\n",
        "        def _backward():\n",
        "            self.inputs[0].gradient += (1 / (np.cos(self.inputs[0].forward())**2)) * self.gradient\n",
        "\n",
        "        self._backward = _backward\n",
        "        \n",
        "    def backward(self):\n",
        "        topo = generate_topo(self)\n",
        "        \n",
        "        self.gradient = 1\n",
        "        for node in reversed(topo):\n",
        "            if hasattr(node, \"_backward\"):\n",
        "                node._backward()\n",
        "                \n",
        "        return self\n",
        "        \n",
        "    def forward(self) -> Union[np.ndarray, SupportsFloat]:\n",
        "        return np.tan(self.inputs[0].forward())\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return \"Tangent({}, g={})\".format(', '.join([str(input) for input in self.inputs]), self.gradient)\n",
        "\n",
        "class Log(DifferentiableValue, OperatorLike):\n",
        "    count = 0\n",
        "\n",
        "    def __init__(self, x: DifferentiableValue):\n",
        "        super().__init__()\n",
        "        _graph.values.append(self)\n",
        "        self.inputs = [x]\n",
        "        Sine.count += 1\n",
        "        \n",
        "        self.gradient = 0\n",
        "\n",
        "        def _backward():\n",
        "            self.inputs[0].gradient += (1 / self.inputs[0].forward()) * self.gradient\n",
        "\n",
        "        self._backward = _backward\n",
        "        \n",
        "    def backward(self):\n",
        "        topo = generate_topo(self)\n",
        "        \n",
        "        self.gradient = 1\n",
        "        for node in reversed(topo):\n",
        "            if hasattr(node, \"_backward\"):\n",
        "                node._backward()\n",
        "                \n",
        "        return self\n",
        "        \n",
        "    def forward(self) -> Union[np.ndarray, SupportsFloat]:\n",
        "        return np.log(abs(self.inputs[0].forward()))\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return \"Log({}, g={})\".format(', '.join([str(input) for input in self.inputs]), self.gradient)\n",
        "\n",
        "class Exp(DifferentiableValue, OperatorLike):\n",
        "    count = 0\n",
        "\n",
        "    def __init__(self, x: DifferentiableValue):\n",
        "        super().__init__()\n",
        "        _graph.values.append(self)\n",
        "        self.inputs = [x]\n",
        "        Sine.count += 1\n",
        "        \n",
        "        self.gradient = 0\n",
        "\n",
        "        def _backward():\n",
        "            self.inputs[0].gradient += (np.exp(self.inputs[0].forward())) * self.gradient\n",
        "\n",
        "        self._backward = _backward\n",
        "        \n",
        "    def backward(self):\n",
        "        topo = generate_topo(self)\n",
        "        \n",
        "        self.gradient = 1\n",
        "        for node in reversed(topo):\n",
        "            if hasattr(node, \"_backward\"):\n",
        "                node._backward()\n",
        "                \n",
        "        return self\n",
        "        \n",
        "    def forward(self) -> Union[np.ndarray, SupportsFloat]:\n",
        "        return np.exp(self.inputs[0].forward())\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return \"Log({}, g={})\".format(', '.join([str(input) for input in self.inputs]), self.gradient)"
      ],
      "metadata": {
        "id": "Sow6R110CKzS"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Activation Functions"
      ],
      "metadata": {
        "id": "O919CcA3Dtl8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReLU(DifferentiableValue, OperatorLike):\n",
        "    count = 0\n",
        "\n",
        "    def __init__(self, x: DifferentiableValue):\n",
        "        super().__init__()\n",
        "        _graph.values.append(self)\n",
        "        self.inputs = [x]\n",
        "        ReLU.count += 1\n",
        "        \n",
        "        self.gradient = 0\n",
        "\n",
        "        def _backward():\n",
        "            self.inputs[0].gradient += (1 if self.inputs[0].forward() > 0 else 0) * self.gradient\n",
        "\n",
        "        self._backward = _backward\n",
        "        \n",
        "    def backward(self):\n",
        "        topo = generate_topo(self)\n",
        "        \n",
        "        self.gradient = 1\n",
        "        for node in reversed(topo):\n",
        "            if hasattr(node, \"_backward\"):\n",
        "                node._backward()\n",
        "                \n",
        "        return self\n",
        "        \n",
        "    def forward(self) -> Union[np.ndarray, SupportsFloat]:\n",
        "        return max(0, self.inputs[0].forward())\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return \"ReLU({}, g={})\".format(', '.join([str(input) for input in self.inputs]), self.gradient)\n",
        "\n",
        "class LeakyReLU(DifferentiableValue, OperatorLike):\n",
        "    count = 0\n",
        "\n",
        "    def __init__(self, x: DifferentiableValue, negative_slope: SupportsFloat = 0.01):\n",
        "        super().__init__()\n",
        "        _graph.values.append(self)\n",
        "        self.inputs = [x]\n",
        "        self.negative_slope = negative_slope\n",
        "        LeakyReLU.count += 1\n",
        "        \n",
        "        self.gradient = 0\n",
        "\n",
        "        def _backward():\n",
        "            self.inputs[0].gradient += (1 if self.inputs[0].forward() >= 0 else self.negative_slope) * self.gradient\n",
        "\n",
        "        self._backward = _backward\n",
        "        \n",
        "    def backward(self):\n",
        "        topo = generate_topo(self)\n",
        "        \n",
        "        self.gradient = 1\n",
        "        for node in reversed(topo):\n",
        "            if hasattr(node, \"_backward\"):\n",
        "                node._backward()\n",
        "                \n",
        "        return self\n",
        "        \n",
        "    def forward(self) -> Union[np.ndarray, SupportsFloat]:\n",
        "        return np.array([self.inputs[0].forward() if self.inputs[0].forward() >= 0 else self.inputs[0].forward() * self.negative_slope])\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return \"LeakyReLU({}, g={})\".format(', '.join([str(input) for input in self.inputs]), self.gradient)\n",
        "\n",
        "class Sigmoid(DifferentiableValue, OperatorLike):\n",
        "    count = 0\n",
        "\n",
        "    def __init__(self, x: DifferentiableValue):\n",
        "        super().__init__()\n",
        "        _graph.values.append(self)\n",
        "        self.inputs = [x]\n",
        "        Sigmoid.count += 1\n",
        "        \n",
        "        self.gradient = 0\n",
        "\n",
        "        def _backward():\n",
        "            x = self.inputs[0].forward()\n",
        "            self.inputs[0].gradient += (self.sigmoid(x) * (1-self.sigmoid(x))) * self.gradient\n",
        "\n",
        "        self._backward = _backward\n",
        "        \n",
        "    def backward(self):\n",
        "        topo = generate_topo(self)\n",
        "        \n",
        "        self.gradient = 1\n",
        "        for node in reversed(topo):\n",
        "            if hasattr(node, \"_backward\"):\n",
        "                node._backward()\n",
        "                \n",
        "        return self\n",
        "    \n",
        "    @staticmethod\n",
        "    def sigmoid(x: np.ndarray) -> np.ndarray:\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "        \n",
        "    def forward(self) -> Union[np.ndarray, SupportsFloat]:\n",
        "        return self.sigmoid(self.inputs[0].forward())\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return \"Sigmoid({}, g={})\".format(', '.join([str(input) for input in self.inputs]), self.gradient)\n",
        "\n",
        "class Softmax(DifferentiableValue, OperatorLike):\n",
        "    count = 0\n",
        "\n",
        "    def __init__(self, *inputs: DifferentiableValue):\n",
        "        super().__init__()\n",
        "        _graph.values.append(self)\n",
        "        self.inputs = inputs\n",
        "        Softmax.count += 1\n",
        "        \n",
        "        self.gradient = 0\n",
        "\n",
        "        def _backward():\n",
        "            softmax = self.softmax()\n",
        "            softmax_vector = softmax.reshape(softmax.shape[0],1)\n",
        "            softmax_matrix = np.tile(softmax_vector,softmax.shape[0])\n",
        "            self.inputs[0].gradient += (np.diag(softmax) - (softmax_matrix * np.transpose(softmax_matrix))) * self.gradient\n",
        "\n",
        "        self._backward = _backward\n",
        "        \n",
        "    def backward(self):\n",
        "        topo = generate_topo(self)\n",
        "        \n",
        "        self.gradient = 1\n",
        "        for node in reversed(topo):\n",
        "            if hasattr(node, \"_backward\"):\n",
        "                node._backward()\n",
        "                \n",
        "        return self\n",
        "\n",
        "    def softmax(self) -> np.ndarray:\n",
        "        denom = sum([np.exp(input.forward()) for input in self.inputs])\n",
        "        return np.array([np.exp(input.forward()) / denom for input in self.inputs])\n",
        "        \n",
        "    def forward(self) -> Union[np.ndarray, SupportsFloat]:\n",
        "        denom = sum([np.exp(input.forward()) for input in self.inputs])\n",
        "        return np.array([np.exp(input.forward()) / denom for input in self.inputs])\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return \"Softmax({}, g={})\".format(', '.join([str(input) for input in self.inputs]), self.gradient)"
      ],
      "metadata": {
        "id": "w8OOJ__bDSRO"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperbolic trig functions"
      ],
      "metadata": {
        "id": "o7Uzqow3MsQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Tanh(DifferentiableValue, OperatorLike):\n",
        "    count = 0\n",
        "\n",
        "    def __init__(self, x: DifferentiableValue):\n",
        "        super().__init__()\n",
        "        _graph.values.append(self)\n",
        "        self.inputs = [x]\n",
        "        Tanh.count += 1\n",
        "        \n",
        "        self.gradient = 0\n",
        "\n",
        "        def _backward():\n",
        "            self.inputs[0].gradient += (1 / (np.cosh(self.inputs[0].forward())**2)) * self.gradient\n",
        "\n",
        "        self._backward = _backward\n",
        "        \n",
        "    def backward(self):\n",
        "        topo = generate_topo(self)\n",
        "        \n",
        "        self.gradient = 1\n",
        "        for node in reversed(topo):\n",
        "            if hasattr(node, \"_backward\"):\n",
        "                node._backward()\n",
        "                \n",
        "        return self\n",
        "        \n",
        "    def forward(self) -> Union[np.ndarray, SupportsFloat]:\n",
        "        return np.tanh(self.inputs[0].forward())\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return \"Tanh({}, g={})\".format(', '.join([str(input) for input in self.inputs]), self.gradient)\n",
        "\n",
        "class Cosh(DifferentiableValue, OperatorLike):\n",
        "    count = 0\n",
        "\n",
        "    def __init__(self, x: DifferentiableValue):\n",
        "        super().__init__()\n",
        "        _graph.values.append(self)\n",
        "        self.inputs = [x]\n",
        "        Cosh.count += 1\n",
        "        \n",
        "        self.gradient = 0\n",
        "\n",
        "        def _backward():\n",
        "            self.inputs[0].gradient += (np.sinh(self.inputs[0].forward())) * self.gradient\n",
        "\n",
        "        self._backward = _backward\n",
        "        \n",
        "    def backward(self):\n",
        "        topo = generate_topo(self)\n",
        "        \n",
        "        self.gradient = 1\n",
        "        for node in reversed(topo):\n",
        "            if hasattr(node, \"_backward\"):\n",
        "                node._backward()\n",
        "                \n",
        "        return self\n",
        "        \n",
        "    def forward(self) -> Union[np.ndarray, SupportsFloat]:\n",
        "        return np.cosh(self.inputs[0].forward())\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return \"Cosh({}, g={})\".format(', '.join([str(input) for input in self.inputs]), self.gradient)\n",
        "\n",
        "class Sinh(DifferentiableValue, OperatorLike):\n",
        "    count = 0\n",
        "\n",
        "    def __init__(self, x: DifferentiableValue):\n",
        "        super().__init__()\n",
        "        _graph.values.append(self)\n",
        "        self.inputs = [x]\n",
        "        Sinh.count += 1\n",
        "        \n",
        "        self.gradient = 0\n",
        "\n",
        "        def _backward():\n",
        "            self.inputs[0].gradient += (np.cosh(self.inputs[0].forward())) * self.gradient\n",
        "\n",
        "        self._backward = _backward\n",
        "        \n",
        "    def backward(self):\n",
        "        topo = generate_topo(self)\n",
        "        \n",
        "        self.gradient = 1\n",
        "        for node in reversed(topo):\n",
        "            if hasattr(node, \"_backward\"):\n",
        "                node._backward()\n",
        "                \n",
        "        return self\n",
        "        \n",
        "    def forward(self) -> Union[np.ndarray, SupportsFloat]:\n",
        "        return np.cosh(self.inputs[0].forward())\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return \"Sinh({}, g={})\".format(', '.join([str(input) for input in self.inputs]), self.gradient)"
      ],
      "metadata": {
        "id": "X8Qi_TcGMulz"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Losses"
      ],
      "metadata": {
        "id": "OaJ_lxm5GfzC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossEntropyLoss:\n",
        "    def __call__(self, outputs: np.ndarray, targets: np.ndarray):\n",
        "        log = np.vectorize(lambda x: Log(x))\n",
        "        term_0 = (-outputs+1) * log(-targets + 1 + 1e-7)\n",
        "        term_1 = outputs * log(targets + 1e-7)\n",
        "        res = list((-term_0+term_1).flatten())\n",
        "        \n",
        "        return np.sum(res)/len(res)\n",
        "\n",
        "class MSELoss:\n",
        "    def __call__(self, outputs: np.ndarray, targets: np.ndarray):\n",
        "        lossv = (targets - outputs) * (targets - outputs)\n",
        "        return np.sum(lossv)/sum(lossv.shape)\n",
        "\n",
        "np_vectorize = np.vectorize(lambda x : Variable(x))"
      ],
      "metadata": {
        "id": "eayb87wTGilt"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a `generate_operation` function to act as a closure and preform runtime \"type replacement\" to convert `SupportsFloat` types into `DifferentiableValue`."
      ],
      "metadata": {
        "id": "6BOAQMjkYCEr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "_6v3wx-uzXXj"
      },
      "outputs": [],
      "source": [
        "def generate_operation(op, self: DifferentiableValue, other: Union[DifferentiableValue, SupportsFloat]):\n",
        "    if isinstance(other, DifferentiableValue):\n",
        "        return op(self, other)\n",
        "    if isinstance(other, (SupportsFloat)):\n",
        "        return op(self, Constant(other))\n",
        "    raise TypeError(f\"Incompatible type for operation: {type(other)}.\")\n",
        "\n",
        "DifferentiableValue.__add__ = lambda self, other: generate_operation(Sum, self, other)\n",
        "DifferentiableValue.__sub__ = lambda self, other: self + -other\n",
        "DifferentiableValue.__neg__ = lambda self: self * -1\n",
        "DifferentiableValue.__mul__ = lambda self, other: generate_operation(Product, self, other)\n",
        "DifferentiableValue.__pow__ = lambda self, other: generate_operation(Power, self, other)\n",
        "DifferentiableValue.__truediv__ = lambda self, other: self * (other ** -1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test case:"
      ],
      "metadata": {
        "id": "BaIiAP3-YXxk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLYOCXDCvsx9",
        "outputId": "f3a211b0-9c90-4bcc-dbdf-4979e184f521"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw Graph:\n",
            "[Log(Variable('x' 0.458, g=0), g=0), Sigmoid(Log(Variable('x' 0.458, g=0), g=0), g=0)]\n",
            "\n",
            "Topological Graph:\n",
            "Variable('x' 0.458, g=0)\n",
            "Log(Variable('x' 0.458, g=0), g=0)\n",
            "Sigmoid(Log(Variable('x' 0.458, g=0), g=0), g=0)\n",
            "\n",
            "Forward value:\n",
            "0.829333216455861\n",
            "\n",
            "\n",
            "Backward graph:\n",
            "[Log(Variable('x' 0.458, g=0.22376127937065252), g=0.141539632538837), Sigmoid(Log(Variable('x' 0.458, g=0.22376127937065252), g=0.141539632538837), g=1)]\n",
            "\n",
            "0.6712838381119576\n"
          ]
        }
      ],
      "source": [
        "with Graph() as graph:\n",
        "    x = Variable(0.458, \"x\")\n",
        "    y = Sigmoid(Exp(x))\n",
        "    \n",
        "print(\"Raw Graph:\")\n",
        "print(graph.values)\n",
        "print()\n",
        "\n",
        "print(\"Topological Graph:\")\n",
        "topo = generate_topo(y)\n",
        "for item in topo:\n",
        "    print(item)\n",
        "print()\n",
        "\n",
        "print(\"Forward value:\")\n",
        "print(graph.forward())\n",
        "print()\n",
        "print()\n",
        "\n",
        "print(\"Backward graph:\")\n",
        "graph.backward()\n",
        "print(graph.values)\n",
        "print()\n",
        "\n",
        "grad_graph = create_gradient_graph(graph.backward())\n",
        "print(grad_graph.wrt(x))\n",
        "\n",
        "del graph"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Network:\n",
        "    def __init__(self, lrate = 1e-2):\n",
        "        self.attrs = dir(self)\n",
        "        self.lrate = lrate\n",
        "        self.layers = []\n",
        "\n",
        "    def register(self):\n",
        "        for attr in dir(self):\n",
        "            if attr not in self.attrs:\n",
        "                if isinstance(getattr(self, attr), NetworkOperation):\n",
        "                    self.layers.append(getattr(self, attr))\n",
        "\n",
        "    def backward(self, grads):\n",
        "        for layer in self.layers:\n",
        "            layer.backward(grads, self.lrate)\n",
        "\n",
        "class NetworkOperation(ABC):\n",
        "    def __init__(self, *vars):\n",
        "        self.vars = vars"
      ],
      "metadata": {
        "id": "SNRlzxfnYo8x"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Linear(NetworkOperation):\n",
        "    def __init__(self, input_features, output_features):\n",
        "        self.input_features = input_features\n",
        "        self.output_features = output_features\n",
        "        self.weights = np_vectorize( np.random.random((input_features, output_features)))\n",
        "        super().__init__(self.weights)\n",
        "\n",
        "    def backward(self, grad, lrate):\n",
        "        grads = grad.wrt(self.weights)\n",
        "        for i, weight in np.ndenumerate(self.weights):\n",
        "            weight.value -= lrate * list(grads)[sum(i)].forward()\n",
        "    \n",
        "    def __call__(self, x):\n",
        "        return np.dot(x, self.weights)"
      ],
      "metadata": {
        "id": "Wka4v60CZS5x"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(Network):\n",
        "    def __init__(self, input_size, mid_size, output_size):\n",
        "        super().__init__()\n",
        "        self.inp = Linear(input_size, mid_size)\n",
        "        self.middle = Linear(mid_size, mid_size)\n",
        "        self.output = Linear(mid_size, output_size)\n",
        "        super().register()\n",
        "\n",
        "    def forward(self, x):\n",
        "        inp = self.inp(x)\n",
        "        mid = self.middle(inp)\n",
        "        return self.output(mid)"
      ],
      "metadata": {
        "id": "UOaLvZY3ZEZ6"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import tqdm\n",
        "np.random.seed(0)\n",
        "\n",
        "input_size = 5\n",
        "mid_size = 6\n",
        "output_size = 10\n",
        "\n",
        "x = np_vectorize(np.random.random(input_size))\n",
        "y_true = np_vectorize(np.random.random(output_size))\n",
        "\n",
        "mod = Model(input_size, mid_size, output_size)\n",
        "criterion = MSELoss()\n",
        "\n",
        "\n",
        "losses = []\n",
        "with Graph() as graph:\n",
        "    for i in tqdm.tqdm(range(75)):\n",
        "        y_pred = mod.forward(x)\n",
        "        loss = criterion(y_true, y_pred)\n",
        "        losses.append(loss.forward())\n",
        "\n",
        "        grad_graph = create_gradient_graph(loss.backward())\n",
        "        mod.backward(grad_graph)\n",
        "\n",
        "plt.plot(losses)\n",
        "plt.xlabel(\"Time step\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"3 linear layer learning\")\n",
        "plt.show()\n",
        "\n",
        "del graph\n",
        "\n",
        "print(\"Final loss: \"+str(losses[-1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "id": "ibTr_E1waWZs",
        "outputId": "f0c303f4-4b35-474c-8c4c-499cdb36f8f2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 75/75 [00:48<00:00,  1.55it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final loss: 0.20670482563217463\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import tqdm\n",
        "np.random.seed(0)\n",
        "\n",
        "input_size = 5\n",
        "output_size = 10\n",
        "lrate = 0.01\n",
        "\n",
        "def update_weights(weights, gradients, lrate):\n",
        "    for i, weight in np.ndenumerate(weights):\n",
        "        weight.value -= lrate * list(gradients)[sum(i)].forward()\n",
        "\n",
        "x = np_vectorize(np.random.random(input_size))\n",
        "y_true = np_vectorize(np.random.random(output_size))\n",
        "weights = np_vectorize(np.random.random((input_size, output_size)))\n",
        "weights2 = np_vectorize(np.random.random((output_size, output_size)))\n",
        "\n",
        "losses = []\n",
        "with Graph() as graph:\n",
        "    for i in tqdm.tqdm(range(10)):\n",
        "        m = np.dot(x, weights)\n",
        "        y_pred = np.dot(m, weights2)\n",
        "        loss = np.sum((y_true - y_pred) * (y_true - y_pred))\n",
        "        losses.append(loss.forward())\n",
        "\n",
        "        grad_graph = create_gradient_graph(loss.backward())\n",
        "        update_weights(weights, grad_graph.wrt(weights), lrate)\n",
        "        update_weights(weights2, grad_graph.wrt(weights2), lrate)\n",
        "\n",
        "plt.plot(losses)\n",
        "plt.xlabel(\"Time step\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Single linear layer learning\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "del graph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "2zmQ5rQVIPUP",
        "outputId": "947a805e-0892-4433-f03a-d57da9debe09"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:06<00:00,  1.64it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}