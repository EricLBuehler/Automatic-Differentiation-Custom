{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EricLBuehler/Automatic-Differentiation-Custom/blob/main/autodiff.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PZkYrhKktTc"
      },
      "source": [
        "#Automatic Differentiation\n",
        "\n",
        "Automatic Differentiation is a core tool used to calculate derivatives which are key to machine learning. This is my personal implementation.\n",
        "\n",
        "Eric Buehler 2023"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "-CAtru2sknMQ"
      },
      "outputs": [],
      "source": [
        "# AD\n",
        "#https://towardsdatascience.com/build-your-own-automatic-differentiation-program-6ecd585eec2a\n",
        "#https://e-dorigatti.github.io/math/deep%20learning/2020/04/07/autodiff.html\n",
        "#https://jingnanshi.com/blog/autodiff.html\n",
        "#https://github.com/karpathy/micrograd\n",
        "#https://sidsite.com/posts/autodiff/\n",
        "\n",
        "# Simple NN implementation\n",
        "#https://mostafa-samir.github.io/auto-diff-pt2/#putting-everything-into-action\n",
        "#https://stackoverflow.com/questions/67615051/implementing-binary-cross-entropy-loss-gives-different-answer-than-tensorflows\n",
        "\n",
        "# Softmax derivative\n",
        "#https://aimatters.wordpress.com/2019/06/17/the-softmax-function-derivative/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "T_UIIcR4pT-T"
      },
      "outputs": [],
      "source": [
        "from abc import ABC, abstractmethod\n",
        "from typing import *\n",
        "from functools import reduce\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a base abstract class for all values, and a general OperatorLike class."
      ],
      "metadata": {
        "id": "geGY6CR4UcaM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "-z244F_fpke4"
      },
      "outputs": [],
      "source": [
        "class DifferentiableValue(ABC):\n",
        "    count = 0\n",
        "    def __init__(self):\n",
        "        DifferentiableValue.count += 1\n",
        "        self.id = DifferentiableValue.count \n",
        "\n",
        "    @abstractmethod\n",
        "    def backward(self, var):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def forward(self):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def __repr__(self) -> str:\n",
        "        pass\n",
        "\n",
        "class OperatorLike(ABC):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "General functions\n",
        "\n",
        "- `generate_topo` topographically sorts a graph, ensuring a directed acyclic graph."
      ],
      "metadata": {
        "id": "5OQJUq07UtJ9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "CJOjSKB2Qi0O"
      },
      "outputs": [],
      "source": [
        "def generate_topo(graph: DifferentiableValue) -> List[DifferentiableValue]:\n",
        "    topo = []\n",
        "    visited = set()\n",
        "    def build_topo(node: DifferentiableValue) -> List[DifferentiableValue]:\n",
        "        if node not in visited:\n",
        "            visited.add(node)\n",
        "            if hasattr(node, \"inputs\"):\n",
        "                for input in node.inputs:\n",
        "                    build_topo(input)\n",
        "            topo.append(node)\n",
        "        return topo\n",
        "    return build_topo(graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a graph to hold the operations, and a gradient graph to hold gradients."
      ],
      "metadata": {
        "id": "oIb72qpYUnJh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "NBTQOAp-rzN4"
      },
      "outputs": [],
      "source": [
        "class GradientGraph:\n",
        "    def __init__(self, graph: List[DifferentiableValue]):\n",
        "        self.graph = graph\n",
        "    \n",
        "    def wrt(self, var: Union[DifferentiableValue, np.ndarray]) -> np.ndarray:\n",
        "        if isinstance(var,  np.ndarray):\n",
        "            array = []\n",
        "            for item in np.nditer(var.flatten(),[\"refs_ok\"]):\n",
        "                if item not in self.graph:\n",
        "                    raise KeyError(f\"Variable {var} not found in gradient graph.\")\n",
        "                \n",
        "                for v in self.graph:\n",
        "                    if v == item:\n",
        "                        array.append(v)\n",
        "            return np.array(array)\n",
        "\n",
        "\n",
        "        if var not in self.graph:\n",
        "            raise KeyError(f\"Variable {var} not found in gradient graph.\")\n",
        "        for v in self.graph:\n",
        "            if v == var:\n",
        "                return v.gradient\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"GradientGraph: {}\".format(\", \".join([str(item) for item in self.graph]))\n",
        "\n",
        "class Graph:\n",
        "    def __init__(self):\n",
        "        self.values = []\n",
        "        global _graph\n",
        "        _graph = self\n",
        "        self.has_backwarded = False\n",
        "        self.clean_graph = lambda values: (values := values[:-1])\n",
        "\n",
        "    def __enter__(self):\n",
        "        return self\n",
        "    \n",
        "    def __exit__(self, exc_type, exc_value, traceback):\n",
        "        pass\n",
        "\n",
        "    def forward(self) -> Union[np.ndarray, SupportsFloat]:\n",
        "        self.clean_graph(self.values)\n",
        "        return self.values[-1].forward()\n",
        "\n",
        "    def backward(self) -> np.ndarray:\n",
        "        self.clean_graph(self.values)\n",
        "        graph = self.values.copy()\n",
        "        res = self.values[-1].backward()\n",
        "        self.values = graph\n",
        "        self.has_backwarded = True\n",
        "        return res\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"Graph: {}\".format(\", \".join([str(item) for item in self.values]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_gradient_graph(item) -> GradientGraph:\n",
        "    filtered = set()\n",
        "    topo = generate_topo(item)\n",
        "    for value in topo:\n",
        "        if not (isinstance(value, OperatorLike) and isinstance(value, Variable)):\n",
        "            filtered.add(value)\n",
        "            \n",
        "    return GradientGraph(list(filtered))"
      ],
      "metadata": {
        "id": "duCL-WEZKZEJ"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define constant and variable values.\n",
        "\n",
        "The `.backward` functions return the derivative of constants and variables."
      ],
      "metadata": {
        "id": "U0ZoBayKWJLf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "jPRsl5tqqcQ4"
      },
      "outputs": [],
      "source": [
        "class Constant(DifferentiableValue):\n",
        "    count = 0\n",
        "\n",
        "    def __init__(self, value: Union[SupportsFloat, np.ndarray]):\n",
        "        super().__init__()\n",
        "        self.value = value\n",
        "        Constant.count += 1\n",
        "        \n",
        "        self.gradient = 0\n",
        "        \n",
        "    def backward(self):\n",
        "        self.gradient = 0\n",
        "                \n",
        "        return _graph\n",
        "        \n",
        "    def forward(self) -> Any:\n",
        "        return self.value\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return f\"Constant({self.value}, g={self.gradient})\"\n",
        "\n",
        "class Variable(DifferentiableValue):\n",
        "    count = 0\n",
        "\n",
        "    def __init__(self, value: Union[SupportsFloat, np.ndarray] = None, name = None):\n",
        "        super().__init__()\n",
        "        self._value = value\n",
        "        self.name = name\n",
        "        Variable.count += 1\n",
        "        \n",
        "        self.gradient = 0\n",
        "        \n",
        "    def backward(self):\n",
        "        self.gradient = 1\n",
        "                \n",
        "        return self\n",
        "    \n",
        "    @property\n",
        "    def value(self):\n",
        "        return self._value\n",
        "    \n",
        "    @value.setter\n",
        "    def value(self, value):\n",
        "        if self.value == None:\n",
        "            raise ValueError(\"Variable does not have value\")\n",
        "        self._value = value\n",
        "        \n",
        "    def forward(self) -> Union[np.ndarray, SupportsFloat]:\n",
        "        if self.value == None:\n",
        "            raise ValueError(\"Variable does not have value\")\n",
        "        return self.value\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return f\"Variable('{self.name}' {self.value}, g={self.gradient})\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define operation \"nodes\" that act like values.\n",
        "\n",
        "The `.backward` functions return the effective value after applying the chain rule, for sums, products, and powers.\n",
        "\n",
        "Note that the `self.gradient = 1` essentially means that the gradient of y with respect to y is 1 (because `self` is the first element of `reversed(topo`).\n",
        "\n",
        "See https://jingnanshi.com/blog/autodiff.html for a great explanation.\n",
        "Note that the equations above table 1 show how the gradients are accumulated, like can be seen in the `_backward` functions."
      ],
      "metadata": {
        "id": "-JBZxTj5WrZ_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "lSeVTXmRx3Nb"
      },
      "outputs": [],
      "source": [
        "class Sum(DifferentiableValue, OperatorLike):\n",
        "    count = 0\n",
        "\n",
        "    def __init__(self, left: DifferentiableValue, right:  DifferentiableValue):\n",
        "        super().__init__()\n",
        "        _graph.values.append(self)\n",
        "        self.inputs = [left, right]\n",
        "        Sum.count += 1\n",
        "        \n",
        "        self.gradient = 0\n",
        "\n",
        "        def _backward():\n",
        "            for input in self.inputs:\n",
        "                input.gradient += self.gradient\n",
        "\n",
        "        self._backward = _backward\n",
        "        \n",
        "    def backward(self):\n",
        "        topo = generate_topo(self)\n",
        "        \n",
        "        self.gradient = 1\n",
        "        for node in reversed(topo):\n",
        "            if hasattr(node, \"_backward\"):\n",
        "                node._backward()\n",
        "                \n",
        "        return self\n",
        "        \n",
        "    def forward(self) -> Union[np.ndarray, SupportsFloat]:\n",
        "        return sum([input.forward() for input in self.inputs])\n",
        "    \n",
        "    def __repr__(self) -> str:\n",
        "        return \"Sum({}, g={})\".format(', '.join([str(input) for input in self.inputs]), self.gradient)\n",
        "\n",
        "class Product(DifferentiableValue, OperatorLike):\n",
        "    count = 0\n",
        "\n",
        "    def __init__(self, left: DifferentiableValue, right:  DifferentiableValue):\n",
        "        super().__init__()\n",
        "        _graph.values.append(self)\n",
        "        self.inputs = [left, right]\n",
        "        Product.count += 1\n",
        "        \n",
        "        self.gradient = 0\n",
        "\n",
        "        def _backward():\n",
        "            self.inputs[0].gradient += self.inputs[1].forward() * self.gradient\n",
        "            self.inputs[1].gradient += self.inputs[0].forward() * self.gradient\n",
        "\n",
        "        self._backward = _backward\n",
        "        \n",
        "    def backward(self):\n",
        "        topo = generate_topo(self)\n",
        "        \n",
        "        self.gradient = 1\n",
        "        for node in reversed(topo):\n",
        "            if hasattr(node, \"_backward\"):\n",
        "                node._backward()\n",
        "                \n",
        "        return self\n",
        "        \n",
        "    def forward(self) -> Union[np.ndarray, SupportsFloat]:\n",
        "        return reduce((lambda x, y: x * y), [input.forward() for input in self.inputs])\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return \"Product({}, g={})\".format(', '.join([str(input) for input in self.inputs]), self.gradient)\n",
        "\n",
        "class Power(DifferentiableValue, OperatorLike):\n",
        "    count = 0\n",
        "\n",
        "    def __init__(self, base: DifferentiableValue, pow:  DifferentiableValue):\n",
        "        super().__init__()\n",
        "        _graph.values.append(self)\n",
        "        self.inputs = [base, pow]\n",
        "        Power.count += 1\n",
        "        \n",
        "        self.gradient = 0\n",
        "\n",
        "        def _backward():\n",
        "            self.inputs[0].gradient += (self.inputs[1].forward() * self.inputs[0].forward() ** (self.inputs[1] - 1).forward()) * self.gradient\n",
        "            self.inputs[1].gradient += abs(np.log(self.inputs[0].forward())) * self.inputs[0].forward() ** (self.inputs[1].forward()) * self.gradient\n",
        "\n",
        "        self._backward = _backward\n",
        "        \n",
        "    def backward(self):\n",
        "        topo = generate_topo(self)\n",
        "        \n",
        "        self.gradient = 1\n",
        "        for node in reversed(topo):\n",
        "            if hasattr(node, \"_backward\"):\n",
        "                node._backward()\n",
        "                \n",
        "        return self\n",
        "        \n",
        "    def forward(self) -> Union[np.ndarray, SupportsFloat]:\n",
        "        return reduce((lambda x, y: x ** y), [input.forward() for input in self.inputs])\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return \"Power({}, g={})\".format(', '.join([str(input) for input in self.inputs]), self.gradient)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trigonometric and other functions"
      ],
      "metadata": {
        "id": "4won-zWcCLf9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Sine(DifferentiableValue, OperatorLike):\n",
        "    count = 0\n",
        "\n",
        "    def __init__(self, x: DifferentiableValue):\n",
        "        super().__init__()\n",
        "        _graph.values.append(self)\n",
        "        self.inputs = [x]\n",
        "        Sine.count += 1\n",
        "        \n",
        "        self.gradient = 0\n",
        "\n",
        "        def _backward():\n",
        "            self.inputs[0].gradient += np.cos(self.inputs[0].forward()) * self.gradient\n",
        "\n",
        "        self._backward = _backward\n",
        "        \n",
        "    def backward(self):\n",
        "        topo = generate_topo(self)\n",
        "        \n",
        "        self.gradient = 1\n",
        "        for node in reversed(topo):\n",
        "            if hasattr(node, \"_backward\"):\n",
        "                node._backward()\n",
        "                \n",
        "        return self\n",
        "        \n",
        "    def forward(self) -> Union[np.ndarray, SupportsFloat]:\n",
        "        return np.sin(self.inputs[0].forward())\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return \"Sine({}, g={})\".format(', '.join([str(input) for input in self.inputs]), self.gradient)\n",
        "\n",
        "class Cosine(DifferentiableValue, OperatorLike):\n",
        "    count = 0\n",
        "\n",
        "    def __init__(self, x: DifferentiableValue):\n",
        "        super().__init__()\n",
        "        _graph.values.append(self)\n",
        "        self.inputs = [x]\n",
        "        Cosine.count += 1\n",
        "        \n",
        "        self.gradient = 0\n",
        "\n",
        "        def _backward():\n",
        "            self.inputs[0].gradient += -np.sin(self.inputs[0].forward()) * self.gradient\n",
        "\n",
        "        self._backward = _backward\n",
        "        \n",
        "    def backward(self):\n",
        "        topo = generate_topo(self)\n",
        "        \n",
        "        self.gradient = 1\n",
        "        for node in reversed(topo):\n",
        "            if hasattr(node, \"_backward\"):\n",
        "                node._backward()\n",
        "                \n",
        "        return self\n",
        "        \n",
        "    def forward(self) -> Union[np.ndarray, SupportsFloat]:\n",
        "        return np.cos(self.inputs[0].forward())\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return \"Cosine({}, g={})\".format(', '.join([str(input) for input in self.inputs]), self.gradient)\n",
        "\n",
        "class Tangent(DifferentiableValue, OperatorLike):\n",
        "    count = 0\n",
        "\n",
        "    def __init__(self, x: DifferentiableValue):\n",
        "        super().__init__()\n",
        "        _graph.values.append(self)\n",
        "        self.inputs = [x]\n",
        "        Tangent.count += 1\n",
        "        \n",
        "        self.gradient = 0\n",
        "\n",
        "        def _backward():\n",
        "            self.inputs[0].gradient += (1 / (np.cos(self.inputs[0].forward())**2)) * self.gradient\n",
        "\n",
        "        self._backward = _backward\n",
        "        \n",
        "    def backward(self):\n",
        "        topo = generate_topo(self)\n",
        "        \n",
        "        self.gradient = 1\n",
        "        for node in reversed(topo):\n",
        "            if hasattr(node, \"_backward\"):\n",
        "                node._backward()\n",
        "                \n",
        "        return self\n",
        "        \n",
        "    def forward(self) -> Union[np.ndarray, SupportsFloat]:\n",
        "        return np.tan(self.inputs[0].forward())\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return \"Tangent({}, g={})\".format(', '.join([str(input) for input in self.inputs]), self.gradient)\n",
        "\n",
        "class Log(DifferentiableValue, OperatorLike):\n",
        "    count = 0\n",
        "\n",
        "    def __init__(self, x: DifferentiableValue):\n",
        "        super().__init__()\n",
        "        _graph.values.append(self)\n",
        "        self.inputs = [x]\n",
        "        Sine.count += 1\n",
        "        \n",
        "        self.gradient = 0\n",
        "\n",
        "        def _backward():\n",
        "            self.inputs[0].gradient += (1 / self.inputs[0].forward()) * self.gradient\n",
        "\n",
        "        self._backward = _backward\n",
        "        \n",
        "    def backward(self):\n",
        "        topo = generate_topo(self)\n",
        "        \n",
        "        self.gradient = 1\n",
        "        for node in reversed(topo):\n",
        "            if hasattr(node, \"_backward\"):\n",
        "                node._backward()\n",
        "                \n",
        "        return self\n",
        "        \n",
        "    def forward(self) -> Union[np.ndarray, SupportsFloat]:\n",
        "        return np.log(abs(self.inputs[0].forward()))\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return \"Log({}, g={})\".format(', '.join([str(input) for input in self.inputs]), self.gradient)\n",
        "\n",
        "class Exp(DifferentiableValue, OperatorLike):\n",
        "    count = 0\n",
        "\n",
        "    def __init__(self, x: DifferentiableValue):\n",
        "        super().__init__()\n",
        "        _graph.values.append(self)\n",
        "        self.inputs = [x]\n",
        "        Sine.count += 1\n",
        "        \n",
        "        self.gradient = 0\n",
        "\n",
        "        def _backward():\n",
        "            self.inputs[0].gradient += (np.exp(self.inputs[0].forward())) * self.gradient\n",
        "\n",
        "        self._backward = _backward\n",
        "        \n",
        "    def backward(self):\n",
        "        topo = generate_topo(self)\n",
        "        \n",
        "        self.gradient = 1\n",
        "        for node in reversed(topo):\n",
        "            if hasattr(node, \"_backward\"):\n",
        "                node._backward()\n",
        "                \n",
        "        return self\n",
        "        \n",
        "    def forward(self) -> Union[np.ndarray, SupportsFloat]:\n",
        "        return np.exp(self.inputs[0].forward())\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return \"Log({}, g={})\".format(', '.join([str(input) for input in self.inputs]), self.gradient)"
      ],
      "metadata": {
        "id": "Sow6R110CKzS"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Activation Functions"
      ],
      "metadata": {
        "id": "O919CcA3Dtl8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReLU(DifferentiableValue, OperatorLike):\n",
        "    count = 0\n",
        "\n",
        "    def __init__(self, x: DifferentiableValue):\n",
        "        super().__init__()\n",
        "        _graph.values.append(self)\n",
        "        self.inputs = [x]\n",
        "        ReLU.count += 1\n",
        "        \n",
        "        self.gradient = 0\n",
        "\n",
        "        def _backward():\n",
        "            self.inputs[0].gradient += (1 if self.inputs[0].forward() > 0 else 0) * self.gradient\n",
        "\n",
        "        self._backward = _backward\n",
        "        \n",
        "    def backward(self):\n",
        "        topo = generate_topo(self)\n",
        "        \n",
        "        self.gradient = 1\n",
        "        for node in reversed(topo):\n",
        "            if hasattr(node, \"_backward\"):\n",
        "                node._backward()\n",
        "                \n",
        "        return self\n",
        "        \n",
        "    def forward(self) -> Union[np.ndarray, SupportsFloat]:\n",
        "        return max(0, self.inputs[0].forward())\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return \"ReLU({}, g={})\".format(', '.join([str(input) for input in self.inputs]), self.gradient)\n",
        "\n",
        "class LeakyReLU(DifferentiableValue, OperatorLike):\n",
        "    count = 0\n",
        "\n",
        "    def __init__(self, x: DifferentiableValue, negative_slope: SupportsFloat = 0.01):\n",
        "        super().__init__()\n",
        "        _graph.values.append(self)\n",
        "        self.inputs = [x]\n",
        "        self.negative_slope = negative_slope\n",
        "        LeakyReLU.count += 1\n",
        "        \n",
        "        self.gradient = 0\n",
        "\n",
        "        def _backward():\n",
        "            self.inputs[0].gradient += (1 if self.inputs[0].forward() >= 0 else self.negative_slope) * self.gradient\n",
        "\n",
        "        self._backward = _backward\n",
        "        \n",
        "    def backward(self):\n",
        "        topo = generate_topo(self)\n",
        "        \n",
        "        self.gradient = 1\n",
        "        for node in reversed(topo):\n",
        "            if hasattr(node, \"_backward\"):\n",
        "                node._backward()\n",
        "                \n",
        "        return self\n",
        "        \n",
        "    def forward(self) -> Union[np.ndarray, SupportsFloat]:\n",
        "        return np.array([self.inputs[0].forward() if self.inputs[0].forward() >= 0 else self.inputs[0].forward() * self.negative_slope])\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return \"LeakyReLU({}, g={})\".format(', '.join([str(input) for input in self.inputs]), self.gradient)\n",
        "\n",
        "class Sigmoid(DifferentiableValue, OperatorLike):\n",
        "    count = 0\n",
        "\n",
        "    def __init__(self, x: DifferentiableValue):\n",
        "        super().__init__()\n",
        "        _graph.values.append(self)\n",
        "        self.inputs = [x]\n",
        "        Sigmoid.count += 1\n",
        "        \n",
        "        self.gradient = 0\n",
        "\n",
        "        def _backward():\n",
        "            x = self.inputs[0].forward()\n",
        "            self.inputs[0].gradient += (self.sigmoid(x) * (1-self.sigmoid(x))) * self.gradient\n",
        "\n",
        "        self._backward = _backward\n",
        "        \n",
        "    def backward(self):\n",
        "        topo = generate_topo(self)\n",
        "        \n",
        "        self.gradient = 1\n",
        "        for node in reversed(topo):\n",
        "            if hasattr(node, \"_backward\"):\n",
        "                node._backward()\n",
        "                \n",
        "        return self\n",
        "    \n",
        "    @staticmethod\n",
        "    def sigmoid(x: np.ndarray) -> np.ndarray:\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "        \n",
        "    def forward(self) -> Union[np.ndarray, SupportsFloat]:\n",
        "        return self.sigmoid(self.inputs[0].forward())\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return \"Sigmoid({}, g={})\".format(', '.join([str(input) for input in self.inputs]), self.gradient)\n",
        "\n",
        "class Softmax(DifferentiableValue, OperatorLike):\n",
        "    count = 0\n",
        "\n",
        "    def __init__(self, *inputs: DifferentiableValue):\n",
        "        super().__init__()\n",
        "        _graph.values.append(self)\n",
        "        self.inputs = inputs\n",
        "        Softmax.count += 1\n",
        "        \n",
        "        self.gradient = 0\n",
        "\n",
        "        def _backward():\n",
        "            softmax = self.softmax()\n",
        "            softmax_vector = softmax.reshape(softmax.shape[0],1)\n",
        "            softmax_matrix = np.tile(softmax_vector,softmax.shape[0])\n",
        "            self.inputs[0].gradient += (np.diag(softmax) - (softmax_matrix * np.transpose(softmax_matrix))) * self.gradient\n",
        "\n",
        "        self._backward = _backward\n",
        "        \n",
        "    def backward(self):\n",
        "        topo = generate_topo(self)\n",
        "        \n",
        "        self.gradient = 1\n",
        "        for node in reversed(topo):\n",
        "            if hasattr(node, \"_backward\"):\n",
        "                node._backward()\n",
        "                \n",
        "        return self\n",
        "\n",
        "    def softmax(self) -> np.ndarray:\n",
        "        denom = sum([np.exp(input.forward()) for input in self.inputs])\n",
        "        return np.array([np.exp(input.forward()) / denom for input in self.inputs])\n",
        "        \n",
        "    def forward(self) -> Union[np.ndarray, SupportsFloat]:\n",
        "        denom = sum([np.exp(input.forward()) for input in self.inputs])\n",
        "        return np.array([np.exp(input.forward()) / denom for input in self.inputs])\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return \"Softmax({}, g={})\".format(', '.join([str(input) for input in self.inputs]), self.gradient)"
      ],
      "metadata": {
        "id": "w8OOJ__bDSRO"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperbolic trig functions"
      ],
      "metadata": {
        "id": "o7Uzqow3MsQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Tanh(DifferentiableValue, OperatorLike):\n",
        "    count = 0\n",
        "\n",
        "    def __init__(self, x: DifferentiableValue):\n",
        "        super().__init__()\n",
        "        _graph.values.append(self)\n",
        "        self.inputs = [x]\n",
        "        Tanh.count += 1\n",
        "        \n",
        "        self.gradient = 0\n",
        "\n",
        "        def _backward():\n",
        "            self.inputs[0].gradient += (1 / (np.cosh(self.inputs[0].forward())**2)) * self.gradient\n",
        "\n",
        "        self._backward = _backward\n",
        "        \n",
        "    def backward(self):\n",
        "        topo = generate_topo(self)\n",
        "        \n",
        "        self.gradient = 1\n",
        "        for node in reversed(topo):\n",
        "            if hasattr(node, \"_backward\"):\n",
        "                node._backward()\n",
        "                \n",
        "        return self\n",
        "        \n",
        "    def forward(self) -> Union[np.ndarray, SupportsFloat]:\n",
        "        return np.tanh(self.inputs[0].forward())\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return \"Tanh({}, g={})\".format(', '.join([str(input) for input in self.inputs]), self.gradient)\n",
        "\n",
        "class Cosh(DifferentiableValue, OperatorLike):\n",
        "    count = 0\n",
        "\n",
        "    def __init__(self, x: DifferentiableValue):\n",
        "        super().__init__()\n",
        "        _graph.values.append(self)\n",
        "        self.inputs = [x]\n",
        "        Cosh.count += 1\n",
        "        \n",
        "        self.gradient = 0\n",
        "\n",
        "        def _backward():\n",
        "            self.inputs[0].gradient += (np.sinh(self.inputs[0].forward())) * self.gradient\n",
        "\n",
        "        self._backward = _backward\n",
        "        \n",
        "    def backward(self):\n",
        "        topo = generate_topo(self)\n",
        "        \n",
        "        self.gradient = 1\n",
        "        for node in reversed(topo):\n",
        "            if hasattr(node, \"_backward\"):\n",
        "                node._backward()\n",
        "                \n",
        "        return self\n",
        "        \n",
        "    def forward(self) -> Union[np.ndarray, SupportsFloat]:\n",
        "        return np.cosh(self.inputs[0].forward())\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return \"Cosh({}, g={})\".format(', '.join([str(input) for input in self.inputs]), self.gradient)\n",
        "\n",
        "class Sinh(DifferentiableValue, OperatorLike):\n",
        "    count = 0\n",
        "\n",
        "    def __init__(self, x: DifferentiableValue):\n",
        "        super().__init__()\n",
        "        _graph.values.append(self)\n",
        "        self.inputs = [x]\n",
        "        Sinh.count += 1\n",
        "        \n",
        "        self.gradient = 0\n",
        "\n",
        "        def _backward():\n",
        "            self.inputs[0].gradient += (np.cosh(self.inputs[0].forward())) * self.gradient\n",
        "\n",
        "        self._backward = _backward\n",
        "        \n",
        "    def backward(self):\n",
        "        topo = generate_topo(self)\n",
        "        \n",
        "        self.gradient = 1\n",
        "        for node in reversed(topo):\n",
        "            if hasattr(node, \"_backward\"):\n",
        "                node._backward()\n",
        "                \n",
        "        return self\n",
        "        \n",
        "    def forward(self) -> Union[np.ndarray, SupportsFloat]:\n",
        "        return np.cosh(self.inputs[0].forward())\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return \"Sinh({}, g={})\".format(', '.join([str(input) for input in self.inputs]), self.gradient)"
      ],
      "metadata": {
        "id": "X8Qi_TcGMulz"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Losses"
      ],
      "metadata": {
        "id": "OaJ_lxm5GfzC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossEntropyLoss:\n",
        "    def __call__(self, outputs: np.ndarray, targets: np.ndarray):\n",
        "        log = np.vectorize(lambda x: Log(x))\n",
        "        term_0 = (-outputs+1) * log(-targets + 1 + 1e-7)\n",
        "        term_1 = outputs * log(targets + 1e-7)\n",
        "        res = list((-term_0+term_1).flatten())\n",
        "        \n",
        "        return np.sum(res)/len(res)\n",
        "\n",
        "class MSELoss:\n",
        "    def __call__(self, outputs: np.ndarray, targets: np.ndarray):\n",
        "        lossv = (targets - outputs) * (targets - outputs)\n",
        "        return np.sum(lossv)/sum(lossv.shape)\n",
        "\n",
        "np_vectorize = np.vectorize(lambda x : Variable(x))"
      ],
      "metadata": {
        "id": "eayb87wTGilt"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a `generate_operation` function to act as a closure and preform runtime \"type replacement\" to convert `SupportsFloat` types into `DifferentiableValue`."
      ],
      "metadata": {
        "id": "6BOAQMjkYCEr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "_6v3wx-uzXXj"
      },
      "outputs": [],
      "source": [
        "def generate_operation(op, self: DifferentiableValue, other: Union[DifferentiableValue, SupportsFloat]):\n",
        "    if isinstance(other, DifferentiableValue):\n",
        "        return op(self, other)\n",
        "    if isinstance(other, (SupportsFloat)):\n",
        "        return op(self, Constant(other))\n",
        "    raise TypeError(f\"Incompatible type for operation: {type(other)}.\")\n",
        "\n",
        "DifferentiableValue.__add__ = lambda self, other: generate_operation(Sum, self, other)\n",
        "DifferentiableValue.__sub__ = lambda self, other: self + -other\n",
        "DifferentiableValue.__neg__ = lambda self: self * -1\n",
        "DifferentiableValue.__mul__ = lambda self, other: generate_operation(Product, self, other)\n",
        "DifferentiableValue.__pow__ = lambda self, other: generate_operation(Power, self, other)\n",
        "DifferentiableValue.__truediv__ = lambda self, other: self * (other ** -1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test case:"
      ],
      "metadata": {
        "id": "BaIiAP3-YXxk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLYOCXDCvsx9",
        "outputId": "15f288db-0a65-4667-dafe-90b29e880e65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw Graph:\n",
            "[Log(Variable('x' 0.458, g=0), g=0), Sigmoid(Log(Variable('x' 0.458, g=0), g=0), g=0)]\n",
            "\n",
            "Topological Graph:\n",
            "Variable('x' 0.458, g=0)\n",
            "Log(Variable('x' 0.458, g=0), g=0)\n",
            "Sigmoid(Log(Variable('x' 0.458, g=0), g=0), g=0)\n",
            "\n",
            "Forward value:\n",
            "0.829333216455861\n",
            "\n",
            "\n",
            "Backward graph:\n",
            "[Log(Variable('x' 0.458, g=0.22376127937065252), g=0.141539632538837), Sigmoid(Log(Variable('x' 0.458, g=0.22376127937065252), g=0.141539632538837), g=1)]\n",
            "\n",
            "0.6712838381119576\n"
          ]
        }
      ],
      "source": [
        "with Graph() as graph:\n",
        "    x = Variable(0.458, \"x\")\n",
        "    y = Sigmoid(Exp(x))\n",
        "    \n",
        "print(\"Raw Graph:\")\n",
        "print(graph.values)\n",
        "print()\n",
        "\n",
        "print(\"Topological Graph:\")\n",
        "topo = generate_topo(y)\n",
        "for item in topo:\n",
        "    print(item)\n",
        "print()\n",
        "\n",
        "print(\"Forward value:\")\n",
        "print(graph.forward())\n",
        "print()\n",
        "print()\n",
        "\n",
        "print(\"Backward graph:\")\n",
        "graph.backward()\n",
        "print(graph.values)\n",
        "print()\n",
        "\n",
        "grad_graph = create_gradient_graph(graph.backward())\n",
        "print(grad_graph.wrt(x))\n",
        "\n",
        "del graph"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Network:\n",
        "    def __init__(self, lrate = 1e-2):\n",
        "        self.attrs = dir(self)\n",
        "        self.lrate = lrate\n",
        "        self.layers = []\n",
        "\n",
        "    def register(self):\n",
        "        for attr in dir(self):\n",
        "            if attr not in self.attrs:\n",
        "                if isinstance(getattr(self, attr), NetworkOperation):\n",
        "                    self.layers.append(getattr(self, attr))\n",
        "\n",
        "    def backward(self, grads):\n",
        "        for layer in self.layers:\n",
        "            layer.backward(grads, self.lrate)\n",
        "\n",
        "class NetworkOperation(ABC):\n",
        "    def __init__(self, *vars):\n",
        "        self.vars = vars"
      ],
      "metadata": {
        "id": "SNRlzxfnYo8x"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Linear(NetworkOperation):\n",
        "    def __init__(self, input_features, output_features, has_bias = True):\n",
        "        self.input_features = input_features\n",
        "        self.output_features = output_features\n",
        "        self.weights = np_vectorize(np.random.random((input_features, output_features)))\n",
        "        self.bias = np_vectorize(np.random.random(output_features)) if has_bias else np_vectorize(np.zeros(output_features))\n",
        "        super().__init__(self.weights)\n",
        "\n",
        "    def backward(self, grad, lrate):\n",
        "        grads = grad.wrt(self.weights)\n",
        "        for i, weight in np.ndenumerate(self.weights):\n",
        "            weight.value -= lrate * list(grads)[sum(i)].forward()\n",
        "\n",
        "        grads = grad.wrt(self.bias)\n",
        "        for i, bias in np.ndenumerate(self.bias):\n",
        "            bias.value -= lrate * list(grads)[sum(i)].forward()\n",
        "    \n",
        "    def __call__(self, x):\n",
        "        return np.add(np.dot(x, self.weights), self.bias)"
      ],
      "metadata": {
        "id": "Wka4v60CZS5x"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(Network):\n",
        "    def __init__(self, input_size, mid_size, output_size):\n",
        "        super().__init__()\n",
        "        self.inp = Linear(input_size, mid_size)\n",
        "        self.middle = Linear(mid_size, mid_size)\n",
        "        self.output = Linear(mid_size, output_size)\n",
        "        super().register()\n",
        "\n",
        "    def forward(self, x):\n",
        "        inp = self.inp(x)\n",
        "        mid = self.middle(inp)\n",
        "        return self.output(mid)"
      ],
      "metadata": {
        "id": "UOaLvZY3ZEZ6"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import tqdm\n",
        "np.random.seed(0)\n",
        "\n",
        "input_size = 5\n",
        "mid_size = 6\n",
        "output_size = 10\n",
        "\n",
        "x = np_vectorize(np.random.random(input_size))\n",
        "y_true = np_vectorize(np.random.random(output_size))\n",
        "\n",
        "mod = Model(input_size, mid_size, output_size)\n",
        "criterion = MSELoss()\n",
        "\n",
        "\n",
        "losses = []\n",
        "with Graph() as graph:\n",
        "    for i in tqdm.tqdm(range(75)):\n",
        "        y_pred = mod.forward(x)\n",
        "        loss = criterion(y_true, y_pred)\n",
        "        losses.append(loss.forward())\n",
        "\n",
        "        grad_graph = create_gradient_graph(loss.backward())\n",
        "        mod.backward(grad_graph)\n",
        "\n",
        "plt.plot(losses)\n",
        "plt.xlabel(\"Time step\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"3 linear layer learning\")\n",
        "plt.show()\n",
        "\n",
        "del graph\n",
        "\n",
        "print(\"Final loss: \"+str(losses[-1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "id": "ibTr_E1waWZs",
        "outputId": "d427360d-846c-4bcb-dc89-17e66611db1b"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 75/75 [00:44<00:00,  1.67it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final loss: 2.5595861776148845\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import tqdm\n",
        "np.random.seed(0)\n",
        "\n",
        "input_size = 5\n",
        "output_size = 10\n",
        "lrate = 0.01\n",
        "\n",
        "def update_weights(weights, gradients, lrate):\n",
        "    for i, weight in np.ndenumerate(weights):\n",
        "        weight.value -= lrate * list(gradients)[sum(i)].forward()\n",
        "\n",
        "x = np_vectorize(np.random.random(input_size))\n",
        "y_true = np_vectorize(np.random.random(output_size))\n",
        "weights = np_vectorize(np.random.random((input_size, output_size)))\n",
        "weights2 = np_vectorize(np.random.random((output_size, output_size)))\n",
        "\n",
        "losses = []\n",
        "with Graph() as graph:\n",
        "    for i in tqdm.tqdm(range(10)):\n",
        "        m = np.dot(x, weights)\n",
        "        y_pred = np.dot(m, weights2)\n",
        "        loss = np.sum((y_true - y_pred) * (y_true - y_pred))\n",
        "        losses.append(loss.forward())\n",
        "\n",
        "        grad_graph = create_gradient_graph(loss.backward())\n",
        "        update_weights(weights, grad_graph.wrt(weights), lrate)\n",
        "        update_weights(weights2, grad_graph.wrt(weights2), lrate)\n",
        "\n",
        "plt.plot(losses)\n",
        "plt.xlabel(\"Time step\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Single linear layer learning\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "del graph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "2zmQ5rQVIPUP",
        "outputId": "8b329c6b-0093-420a-d03e-2ebd0b691263"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:05<00:00,  1.86it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}