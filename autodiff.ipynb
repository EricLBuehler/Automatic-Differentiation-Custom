{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EricLBuehler/Automatic-Differentiation-Custom/blob/main/autodiff.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PZkYrhKktTc"
      },
      "source": [
        "#Automatic Differentiation\n",
        "\n",
        "Automatic Differentiation is a core tool used to calculate derivatives which are key to machine learning. This is my personal implementation.\n",
        "\n",
        "Eric Buehler 2023"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "-CAtru2sknMQ"
      },
      "outputs": [],
      "source": [
        "# AD\n",
        "#https://towardsdatascience.com/build-your-own-automatic-differentiation-program-6ecd585eec2a\n",
        "#https://e-dorigatti.github.io/math/deep%20learning/2020/04/07/autodiff.html\n",
        "#https://jingnanshi.com/blog/autodiff.html\n",
        "#https://github.com/karpathy/micrograd\n",
        "#https://sidsite.com/posts/autodiff/\n",
        "\n",
        "# Simple NN implementation\n",
        "#https://mostafa-samir.github.io/auto-diff-pt2/#putting-everything-into-action\n",
        "#https://stackoverflow.com/questions/67615051/implementing-binary-cross-entropy-loss-gives-different-answer-than-tensorflows\n",
        "\n",
        "# Softmax derivative\n",
        "#https://aimatters.wordpress.com/2019/06/17/the-softmax-function-derivative/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "T_UIIcR4pT-T"
      },
      "outputs": [],
      "source": [
        "from abc import ABC, abstractmethod\n",
        "from typing import *\n",
        "from functools import reduce\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a base abstract class for all values, and a general OperatorLike class."
      ],
      "metadata": {
        "id": "geGY6CR4UcaM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "-z244F_fpke4"
      },
      "outputs": [],
      "source": [
        "class DifferentiableValue(ABC):\n",
        "    count = 0\n",
        "    def __init__(self):\n",
        "        DifferentiableValue.count += 1\n",
        "        self.id = DifferentiableValue.count \n",
        "\n",
        "    @abstractmethod\n",
        "    def backward(self, var):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def forward(self):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def __repr__(self) -> str:\n",
        "        pass\n",
        "\n",
        "class OperatorLike(ABC):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "General functions\n",
        "\n",
        "- `generate_topo` topographically sorts a graph, ensuring a directed acyclic graph."
      ],
      "metadata": {
        "id": "5OQJUq07UtJ9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "CJOjSKB2Qi0O"
      },
      "outputs": [],
      "source": [
        "def generate_topo(graph: DifferentiableValue) -> List[DifferentiableValue]:\n",
        "    topo = []\n",
        "    visited = set()\n",
        "    def build_topo(node: DifferentiableValue) -> List[DifferentiableValue]:\n",
        "        if node not in visited:\n",
        "            visited.add(node)\n",
        "            if hasattr(node, \"inputs\"):\n",
        "                for input in node.inputs:\n",
        "                    build_topo(input)\n",
        "            topo.append(node)\n",
        "        return topo\n",
        "    return build_topo(graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a graph to hold the operations, and a gradient graph to hold gradients."
      ],
      "metadata": {
        "id": "oIb72qpYUnJh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "NBTQOAp-rzN4"
      },
      "outputs": [],
      "source": [
        "class GradientGraph:\n",
        "    def __init__(self, graph: List[DifferentiableValue]):\n",
        "        self.graph = graph\n",
        "    \n",
        "    def wrt(self, var: Union[DifferentiableValue, np.ndarray]) -> np.ndarray:\n",
        "        if isinstance(var,  np.ndarray):\n",
        "            array = []\n",
        "            for item in np.nditer(var.flatten(),[\"refs_ok\"]):\n",
        "                if item not in self.graph:\n",
        "                    raise KeyError(f\"Variable {var} not found in gradient graph.\")\n",
        "                \n",
        "                for v in self.graph:\n",
        "                    if v == item:\n",
        "                        array.append(v)\n",
        "            return np.array(array)\n",
        "\n",
        "\n",
        "        if var not in self.graph:\n",
        "            raise KeyError(f\"Variable {var} not found in gradient graph.\")\n",
        "        for v in self.graph:\n",
        "            if v == var:\n",
        "                return v.gradient\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"GradientGraph: {}\".format(\", \".join([str(item) for item in self.graph]))\n",
        "\n",
        "class Graph:\n",
        "    def __init__(self):\n",
        "        self.values = []\n",
        "        global _graph\n",
        "        _graph = self\n",
        "        self.has_backwarded = False\n",
        "        self.clean_graph = lambda values: (values := values[:-1])\n",
        "\n",
        "    def __enter__(self):\n",
        "        return self\n",
        "    \n",
        "    def __exit__(self, exc_type, exc_value, traceback):\n",
        "        pass\n",
        "\n",
        "    def forward(self) -> Union[np.ndarray, SupportsFloat]:\n",
        "        self.clean_graph(self.values)\n",
        "        return self.values[-1].forward()\n",
        "\n",
        "    def backward(self) -> np.ndarray:\n",
        "        self.clean_graph(self.values)\n",
        "        graph = self.values.copy()\n",
        "        res = self.values[-1].backward()\n",
        "        self.values = graph\n",
        "        self.has_backwarded = True\n",
        "        return res\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"Graph: {}\".format(\", \".join([str(item) for item in self.values]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_gradient_graph(item) -> GradientGraph:\n",
        "    filtered = set()\n",
        "    topo = generate_topo(item)\n",
        "    for value in topo:\n",
        "        if not (isinstance(value, OperatorLike) and isinstance(value, Variable)):\n",
        "            filtered.add(value)\n",
        "            \n",
        "    return GradientGraph(list(filtered))"
      ],
      "metadata": {
        "id": "duCL-WEZKZEJ"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define constant and variable values.\n",
        "\n",
        "The `.backward` functions return the derivative of constants and variables."
      ],
      "metadata": {
        "id": "U0ZoBayKWJLf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "jPRsl5tqqcQ4"
      },
      "outputs": [],
      "source": [
        "class Constant(DifferentiableValue):\n",
        "    count = 0\n",
        "\n",
        "    def __init__(self, value: Union[SupportsFloat, np.ndarray]):\n",
        "        super().__init__()\n",
        "        self.value = value\n",
        "        Constant.count += 1\n",
        "        \n",
        "        self.gradient = 0\n",
        "        \n",
        "    def backward(self):\n",
        "        self.gradient = 0\n",
        "                \n",
        "        return _graph\n",
        "        \n",
        "    def forward(self) -> Any:\n",
        "        return self.value\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return f\"Constant({self.value}, g={self.gradient})\"\n",
        "\n",
        "class Variable(DifferentiableValue):\n",
        "    count = 0\n",
        "\n",
        "    def __init__(self, value: Union[SupportsFloat, np.ndarray] = None, name = None):\n",
        "        super().__init__()\n",
        "        self._value = value\n",
        "        self.name = name\n",
        "        Variable.count += 1\n",
        "        \n",
        "        self.gradient = 0\n",
        "        \n",
        "    def backward(self):\n",
        "        self.gradient = 1\n",
        "                \n",
        "        return self\n",
        "    \n",
        "    @property\n",
        "    def value(self):\n",
        "        return self._value\n",
        "    \n",
        "    @value.setter\n",
        "    def value(self, value):\n",
        "        if self.value == None:\n",
        "            raise ValueError(\"Variable does not have value\")\n",
        "        self._value = value\n",
        "        \n",
        "    def forward(self) -> Union[np.ndarray, SupportsFloat]:\n",
        "        if self.value == None:\n",
        "            raise ValueError(\"Variable does not have value\")\n",
        "        return self.value\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return f\"Variable('{self.name}' {self.value}, g={self.gradient})\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define operation \"nodes\" that act like values.\n",
        "\n",
        "The `.backward` functions return the effective value after applying the chain rule, for sums, products, and powers.\n",
        "\n",
        "Note that the `self.gradient = 1` essentially means that the gradient of y with respect to y is 1 (because `self` is the first element of `reversed(topo`).\n",
        "\n",
        "See https://jingnanshi.com/blog/autodiff.html for a great explanation.\n",
        "Note that the equations above table 1 show how the gradients are accumulated, like can be seen in the `_backward` functions."
      ],
      "metadata": {
        "id": "-JBZxTj5WrZ_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "lSeVTXmRx3Nb"
      },
      "outputs": [],
      "source": [
        "class Sum(DifferentiableValue, OperatorLike):\n",
        "    count = 0\n",
        "\n",
        "    def __init__(self, left: DifferentiableValue, right:  DifferentiableValue):\n",
        "        super().__init__()\n",
        "        _graph.values.append(self)\n",
        "        self.inputs = [left, right]\n",
        "        Sum.count += 1\n",
        "        \n",
        "        self.gradient = 0\n",
        "\n",
        "        def _backward():\n",
        "            for input in self.inputs:\n",
        "                input.gradient += self.gradient\n",
        "\n",
        "        self._backward = _backward\n",
        "        \n",
        "    def backward(self):\n",
        "        topo = generate_topo(self)\n",
        "        \n",
        "        self.gradient = 1\n",
        "        for node in reversed(topo):\n",
        "            if hasattr(node, \"_backward\"):\n",
        "                node._backward()\n",
        "                \n",
        "        return self\n",
        "        \n",
        "    def forward(self) -> Union[np.ndarray, SupportsFloat]:\n",
        "        return sum([input.forward() for input in self.inputs])\n",
        "    \n",
        "    def __repr__(self) -> str:\n",
        "        return \"Sum({}, g={})\".format(', '.join([str(input) for input in self.inputs]), self.gradient)\n",
        "\n",
        "class Product(DifferentiableValue, OperatorLike):\n",
        "    count = 0\n",
        "\n",
        "    def __init__(self, left: DifferentiableValue, right:  DifferentiableValue):\n",
        "        super().__init__()\n",
        "        _graph.values.append(self)\n",
        "        self.inputs = [left, right]\n",
        "        Product.count += 1\n",
        "        \n",
        "        self.gradient = 0\n",
        "\n",
        "        def _backward():\n",
        "            self.inputs[0].gradient += self.inputs[1].forward() * self.gradient\n",
        "            self.inputs[1].gradient += self.inputs[0].forward() * self.gradient\n",
        "\n",
        "        self._backward = _backward\n",
        "        \n",
        "    def backward(self):\n",
        "        topo = generate_topo(self)\n",
        "        \n",
        "        self.gradient = 1\n",
        "        for node in reversed(topo):\n",
        "            if hasattr(node, \"_backward\"):\n",
        "                node._backward()\n",
        "                \n",
        "        return self\n",
        "        \n",
        "    def forward(self) -> Union[np.ndarray, SupportsFloat]:\n",
        "        return reduce((lambda x, y: x * y), [input.forward() for input in self.inputs])\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return \"Product({}, g={})\".format(', '.join([str(input) for input in self.inputs]), self.gradient)\n",
        "\n",
        "class Power(DifferentiableValue, OperatorLike):\n",
        "    count = 0\n",
        "\n",
        "    def __init__(self, base: DifferentiableValue, pow:  DifferentiableValue):\n",
        "        super().__init__()\n",
        "        _graph.values.append(self)\n",
        "        self.inputs = [base, pow]\n",
        "        Power.count += 1\n",
        "        \n",
        "        self.gradient = 0\n",
        "\n",
        "        def _backward():\n",
        "            self.inputs[0].gradient += (self.inputs[1].forward() * self.inputs[0].forward() ** (self.inputs[1] - 1).forward()) * self.gradient\n",
        "            self.inputs[1].gradient += abs(np.log(self.inputs[0].forward())) * self.inputs[0].forward() ** (self.inputs[1].forward()) * self.gradient\n",
        "\n",
        "        self._backward = _backward\n",
        "        \n",
        "    def backward(self):\n",
        "        topo = generate_topo(self)\n",
        "        \n",
        "        self.gradient = 1\n",
        "        for node in reversed(topo):\n",
        "            if hasattr(node, \"_backward\"):\n",
        "                node._backward()\n",
        "                \n",
        "        return self\n",
        "        \n",
        "    def forward(self) -> Union[np.ndarray, SupportsFloat]:\n",
        "        return reduce((lambda x, y: x ** y), [input.forward() for input in self.inputs])\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return \"Power({}, g={})\".format(', '.join([str(input) for input in self.inputs]), self.gradient)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trigonometric and other functions"
      ],
      "metadata": {
        "id": "4won-zWcCLf9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Sine(DifferentiableValue, OperatorLike):\n",
        "    count = 0\n",
        "\n",
        "    def __init__(self, x: DifferentiableValue):\n",
        "        super().__init__()\n",
        "        _graph.values.append(self)\n",
        "        self.inputs = [x]\n",
        "        Sine.count += 1\n",
        "        \n",
        "        self.gradient = 0\n",
        "\n",
        "        def _backward():\n",
        "            self.inputs[0].gradient += np.cos(self.inputs[0].forward()) * self.gradient\n",
        "\n",
        "        self._backward = _backward\n",
        "        \n",
        "    def backward(self):\n",
        "        topo = generate_topo(self)\n",
        "        \n",
        "        self.gradient = 1\n",
        "        for node in reversed(topo):\n",
        "            if hasattr(node, \"_backward\"):\n",
        "                node._backward()\n",
        "                \n",
        "        return self\n",
        "        \n",
        "    def forward(self) -> Union[np.ndarray, SupportsFloat]:\n",
        "        return np.sin(self.inputs[0].forward())\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return \"Sine({}, g={})\".format(', '.join([str(input) for input in self.inputs]), self.gradient)\n",
        "\n",
        "class Cosine(DifferentiableValue, OperatorLike):\n",
        "    count = 0\n",
        "\n",
        "    def __init__(self, x: DifferentiableValue):\n",
        "        super().__init__()\n",
        "        _graph.values.append(self)\n",
        "        self.inputs = [x]\n",
        "        Cosine.count += 1\n",
        "        \n",
        "        self.gradient = 0\n",
        "\n",
        "        def _backward():\n",
        "            self.inputs[0].gradient += -np.sin(self.inputs[0].forward()) * self.gradient\n",
        "\n",
        "        self._backward = _backward\n",
        "        \n",
        "    def backward(self):\n",
        "        topo = generate_topo(self)\n",
        "        \n",
        "        self.gradient = 1\n",
        "        for node in reversed(topo):\n",
        "            if hasattr(node, \"_backward\"):\n",
        "                node._backward()\n",
        "                \n",
        "        return self\n",
        "        \n",
        "    def forward(self) -> Union[np.ndarray, SupportsFloat]:\n",
        "        return np.cos(self.inputs[0].forward())\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return \"Cosine({}, g={})\".format(', '.join([str(input) for input in self.inputs]), self.gradient)\n",
        "\n",
        "class Tangent(DifferentiableValue, OperatorLike):\n",
        "    count = 0\n",
        "\n",
        "    def __init__(self, x: DifferentiableValue):\n",
        "        super().__init__()\n",
        "        _graph.values.append(self)\n",
        "        self.inputs = [x]\n",
        "        Tangent.count += 1\n",
        "        \n",
        "        self.gradient = 0\n",
        "\n",
        "        def _backward():\n",
        "            self.inputs[0].gradient += (1 / (np.cos(self.inputs[0].forward())**2)) * self.gradient\n",
        "\n",
        "        self._backward = _backward\n",
        "        \n",
        "    def backward(self):\n",
        "        topo = generate_topo(self)\n",
        "        \n",
        "        self.gradient = 1\n",
        "        for node in reversed(topo):\n",
        "            if hasattr(node, \"_backward\"):\n",
        "                node._backward()\n",
        "                \n",
        "        return self\n",
        "        \n",
        "    def forward(self) -> Union[np.ndarray, SupportsFloat]:\n",
        "        return np.tan(self.inputs[0].forward())\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return \"Tangent({}, g={})\".format(', '.join([str(input) for input in self.inputs]), self.gradient)\n",
        "\n",
        "class Log(DifferentiableValue, OperatorLike):\n",
        "    count = 0\n",
        "\n",
        "    def __init__(self, x: DifferentiableValue):\n",
        "        super().__init__()\n",
        "        _graph.values.append(self)\n",
        "        self.inputs = [x]\n",
        "        Sine.count += 1\n",
        "        \n",
        "        self.gradient = 0\n",
        "\n",
        "        def _backward():\n",
        "            self.inputs[0].gradient += (1 / self.inputs[0].forward()) * self.gradient\n",
        "\n",
        "        self._backward = _backward\n",
        "        \n",
        "    def backward(self):\n",
        "        topo = generate_topo(self)\n",
        "        \n",
        "        self.gradient = 1\n",
        "        for node in reversed(topo):\n",
        "            if hasattr(node, \"_backward\"):\n",
        "                node._backward()\n",
        "                \n",
        "        return self\n",
        "        \n",
        "    def forward(self) -> Union[np.ndarray, SupportsFloat]:\n",
        "        return np.log(abs(self.inputs[0].forward()))\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return \"Log({}, g={})\".format(', '.join([str(input) for input in self.inputs]), self.gradient)\n",
        "\n",
        "class Exp(DifferentiableValue, OperatorLike):\n",
        "    count = 0\n",
        "\n",
        "    def __init__(self, x: DifferentiableValue):\n",
        "        super().__init__()\n",
        "        _graph.values.append(self)\n",
        "        self.inputs = [x]\n",
        "        Sine.count += 1\n",
        "        \n",
        "        self.gradient = 0\n",
        "\n",
        "        def _backward():\n",
        "            self.inputs[0].gradient += (np.exp(self.inputs[0].forward())) * self.gradient\n",
        "\n",
        "        self._backward = _backward\n",
        "        \n",
        "    def backward(self):\n",
        "        topo = generate_topo(self)\n",
        "        \n",
        "        self.gradient = 1\n",
        "        for node in reversed(topo):\n",
        "            if hasattr(node, \"_backward\"):\n",
        "                node._backward()\n",
        "                \n",
        "        return self\n",
        "        \n",
        "    def forward(self) -> Union[np.ndarray, SupportsFloat]:\n",
        "        return np.exp(self.inputs[0].forward())\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return \"Log({}, g={})\".format(', '.join([str(input) for input in self.inputs]), self.gradient)"
      ],
      "metadata": {
        "id": "Sow6R110CKzS"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Activation Functions"
      ],
      "metadata": {
        "id": "O919CcA3Dtl8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReLU(DifferentiableValue, OperatorLike):\n",
        "    count = 0\n",
        "\n",
        "    def __init__(self, x: DifferentiableValue):\n",
        "        super().__init__()\n",
        "        _graph.values.append(self)\n",
        "        self.inputs = [x]\n",
        "        ReLU.count += 1\n",
        "        \n",
        "        self.gradient = 0\n",
        "\n",
        "        def _backward():\n",
        "            self.inputs[0].gradient += (1 if self.inputs[0].forward() > 0 else 0) * self.gradient\n",
        "\n",
        "        self._backward = _backward\n",
        "        \n",
        "    def backward(self):\n",
        "        topo = generate_topo(self)\n",
        "        \n",
        "        self.gradient = 1\n",
        "        for node in reversed(topo):\n",
        "            if hasattr(node, \"_backward\"):\n",
        "                node._backward()\n",
        "                \n",
        "        return self\n",
        "        \n",
        "    def forward(self) -> Union[np.ndarray, SupportsFloat]:\n",
        "        return max(0, self.inputs[0].forward())\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return \"ReLU({}, g={})\".format(', '.join([str(input) for input in self.inputs]), self.gradient)\n",
        "\n",
        "class LeakyReLU(DifferentiableValue, OperatorLike):\n",
        "    count = 0\n",
        "\n",
        "    def __init__(self, x: DifferentiableValue, negative_slope: SupportsFloat = 0.01):\n",
        "        super().__init__()\n",
        "        _graph.values.append(self)\n",
        "        self.inputs = [x]\n",
        "        self.negative_slope = negative_slope\n",
        "        LeakyReLU.count += 1\n",
        "        \n",
        "        self.gradient = 0\n",
        "\n",
        "        def _backward():\n",
        "            self.inputs[0].gradient += (1 if self.inputs[0].forward() >= 0 else self.negative_slope) * self.gradient\n",
        "\n",
        "        self._backward = _backward\n",
        "        \n",
        "    def backward(self):\n",
        "        topo = generate_topo(self)\n",
        "        \n",
        "        self.gradient = 1\n",
        "        for node in reversed(topo):\n",
        "            if hasattr(node, \"_backward\"):\n",
        "                node._backward()\n",
        "                \n",
        "        return self\n",
        "        \n",
        "    def forward(self) -> Union[np.ndarray, SupportsFloat]:\n",
        "        return np.array([self.inputs[0].forward() if self.inputs[0].forward() >= 0 else self.inputs[0].forward() * self.negative_slope])\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return \"LeakyReLU({}, g={})\".format(', '.join([str(input) for input in self.inputs]), self.gradient)\n",
        "\n",
        "class Sigmoid(DifferentiableValue, OperatorLike):\n",
        "    count = 0\n",
        "\n",
        "    def __init__(self, x: DifferentiableValue):\n",
        "        super().__init__()\n",
        "        _graph.values.append(self)\n",
        "        self.inputs = [x]\n",
        "        Sigmoid.count += 1\n",
        "        \n",
        "        self.gradient = 0\n",
        "\n",
        "        def _backward():\n",
        "            x = self.inputs[0].forward()\n",
        "            self.inputs[0].gradient += (self.sigmoid(x) * (1-self.sigmoid(x))) * self.gradient\n",
        "\n",
        "        self._backward = _backward\n",
        "        \n",
        "    def backward(self):\n",
        "        topo = generate_topo(self)\n",
        "        \n",
        "        self.gradient = 1\n",
        "        for node in reversed(topo):\n",
        "            if hasattr(node, \"_backward\"):\n",
        "                node._backward()\n",
        "                \n",
        "        return self\n",
        "    \n",
        "    @staticmethod\n",
        "    def sigmoid(x: np.ndarray) -> np.ndarray:\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "        \n",
        "    def forward(self) -> Union[np.ndarray, SupportsFloat]:\n",
        "        return self.sigmoid(self.inputs[0].forward())\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return \"Sigmoid({}, g={})\".format(', '.join([str(input) for input in self.inputs]), self.gradient)\n",
        "\n",
        "class Softmax(DifferentiableValue, OperatorLike):\n",
        "    count = 0\n",
        "\n",
        "    def __init__(self, *inputs: DifferentiableValue):\n",
        "        super().__init__()\n",
        "        _graph.values.append(self)\n",
        "        self.inputs = inputs\n",
        "        Softmax.count += 1\n",
        "        \n",
        "        self.gradient = 0\n",
        "\n",
        "        def _backward():\n",
        "            softmax = self.softmax()\n",
        "            softmax_vector = softmax.reshape(softmax.shape[0],1)\n",
        "            softmax_matrix = np.tile(softmax_vector,softmax.shape[0])\n",
        "            self.inputs[0].gradient += (np.diag(softmax) - (softmax_matrix * np.transpose(softmax_matrix))) * self.gradient\n",
        "\n",
        "        self._backward = _backward\n",
        "        \n",
        "    def backward(self):\n",
        "        topo = generate_topo(self)\n",
        "        \n",
        "        self.gradient = 1\n",
        "        for node in reversed(topo):\n",
        "            if hasattr(node, \"_backward\"):\n",
        "                node._backward()\n",
        "                \n",
        "        return self\n",
        "\n",
        "    def softmax(self) -> np.ndarray:\n",
        "        denom = sum([np.exp(input.forward()) for input in self.inputs])\n",
        "        return np.array([np.exp(input.forward()) / denom for input in self.inputs])\n",
        "        \n",
        "    def forward(self) -> Union[np.ndarray, SupportsFloat]:\n",
        "        denom = sum([np.exp(input.forward()) for input in self.inputs])\n",
        "        return np.array([np.exp(input.forward()) / denom for input in self.inputs])\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return \"Softmax({}, g={})\".format(', '.join([str(input) for input in self.inputs]), self.gradient)"
      ],
      "metadata": {
        "id": "w8OOJ__bDSRO"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperbolic trig functions"
      ],
      "metadata": {
        "id": "o7Uzqow3MsQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Tanh(DifferentiableValue, OperatorLike):\n",
        "    count = 0\n",
        "\n",
        "    def __init__(self, x: DifferentiableValue):\n",
        "        super().__init__()\n",
        "        _graph.values.append(self)\n",
        "        self.inputs = [x]\n",
        "        Tanh.count += 1\n",
        "        \n",
        "        self.gradient = 0\n",
        "\n",
        "        def _backward():\n",
        "            self.inputs[0].gradient += (1 / (np.cosh(self.inputs[0].forward())**2)) * self.gradient\n",
        "\n",
        "        self._backward = _backward\n",
        "        \n",
        "    def backward(self):\n",
        "        topo = generate_topo(self)\n",
        "        \n",
        "        self.gradient = 1\n",
        "        for node in reversed(topo):\n",
        "            if hasattr(node, \"_backward\"):\n",
        "                node._backward()\n",
        "                \n",
        "        return self\n",
        "        \n",
        "    def forward(self) -> Union[np.ndarray, SupportsFloat]:\n",
        "        return np.tanh(self.inputs[0].forward())\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return \"Tanh({}, g={})\".format(', '.join([str(input) for input in self.inputs]), self.gradient)\n",
        "\n",
        "class Cosh(DifferentiableValue, OperatorLike):\n",
        "    count = 0\n",
        "\n",
        "    def __init__(self, x: DifferentiableValue):\n",
        "        super().__init__()\n",
        "        _graph.values.append(self)\n",
        "        self.inputs = [x]\n",
        "        Cosh.count += 1\n",
        "        \n",
        "        self.gradient = 0\n",
        "\n",
        "        def _backward():\n",
        "            self.inputs[0].gradient += (np.sinh(self.inputs[0].forward())) * self.gradient\n",
        "\n",
        "        self._backward = _backward\n",
        "        \n",
        "    def backward(self):\n",
        "        topo = generate_topo(self)\n",
        "        \n",
        "        self.gradient = 1\n",
        "        for node in reversed(topo):\n",
        "            if hasattr(node, \"_backward\"):\n",
        "                node._backward()\n",
        "                \n",
        "        return self\n",
        "        \n",
        "    def forward(self) -> Union[np.ndarray, SupportsFloat]:\n",
        "        return np.cosh(self.inputs[0].forward())\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return \"Cosh({}, g={})\".format(', '.join([str(input) for input in self.inputs]), self.gradient)\n",
        "\n",
        "class Sinh(DifferentiableValue, OperatorLike):\n",
        "    count = 0\n",
        "\n",
        "    def __init__(self, x: DifferentiableValue):\n",
        "        super().__init__()\n",
        "        _graph.values.append(self)\n",
        "        self.inputs = [x]\n",
        "        Sinh.count += 1\n",
        "        \n",
        "        self.gradient = 0\n",
        "\n",
        "        def _backward():\n",
        "            self.inputs[0].gradient += (np.cosh(self.inputs[0].forward())) * self.gradient\n",
        "\n",
        "        self._backward = _backward\n",
        "        \n",
        "    def backward(self):\n",
        "        topo = generate_topo(self)\n",
        "        \n",
        "        self.gradient = 1\n",
        "        for node in reversed(topo):\n",
        "            if hasattr(node, \"_backward\"):\n",
        "                node._backward()\n",
        "                \n",
        "        return self\n",
        "        \n",
        "    def forward(self) -> Union[np.ndarray, SupportsFloat]:\n",
        "        return np.cosh(self.inputs[0].forward())\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return \"Sinh({}, g={})\".format(', '.join([str(input) for input in self.inputs]), self.gradient)"
      ],
      "metadata": {
        "id": "X8Qi_TcGMulz"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Losses"
      ],
      "metadata": {
        "id": "OaJ_lxm5GfzC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossEntropyLoss:\n",
        "    def __call__(self, outputs: np.ndarray, targets: np.ndarray):\n",
        "        log = np.vectorize(lambda x: Log(x))\n",
        "        term_0 = (-outputs+1) * log(-targets + 1 + 1e-7)\n",
        "        term_1 = outputs * log(targets + 1e-7)\n",
        "        res = list((-term_0+term_1).flatten())\n",
        "        \n",
        "        return np.sum(res)/len(res)\n",
        "\n",
        "class MSELoss:\n",
        "    def __call__(self, outputs: np.ndarray, targets: np.ndarray):\n",
        "        lossv = (targets - outputs) * (targets - outputs)\n",
        "        return np.sum(lossv)/sum(lossv.shape)\n",
        "\n",
        "np_vectorize = np.vectorize(lambda x : Variable(x))"
      ],
      "metadata": {
        "id": "eayb87wTGilt"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a `generate_operation` function to act as a closure and preform runtime \"type replacement\" to convert `SupportsFloat` types into `DifferentiableValue`."
      ],
      "metadata": {
        "id": "6BOAQMjkYCEr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "_6v3wx-uzXXj"
      },
      "outputs": [],
      "source": [
        "def generate_operation(op, self: DifferentiableValue, other: Union[DifferentiableValue, SupportsFloat]):\n",
        "    if isinstance(other, DifferentiableValue):\n",
        "        return op(self, other)\n",
        "    if isinstance(other, (SupportsFloat)):\n",
        "        return op(self, Constant(other))\n",
        "    raise TypeError(f\"Incompatible type for operation: {type(other)}.\")\n",
        "\n",
        "DifferentiableValue.__add__ = lambda self, other: generate_operation(Sum, self, other)\n",
        "DifferentiableValue.__sub__ = lambda self, other: self + -other\n",
        "DifferentiableValue.__neg__ = lambda self: self * -1\n",
        "DifferentiableValue.__mul__ = lambda self, other: generate_operation(Product, self, other)\n",
        "DifferentiableValue.__pow__ = lambda self, other: generate_operation(Power, self, other)\n",
        "DifferentiableValue.__truediv__ = lambda self, other: self * (other ** -1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test case:"
      ],
      "metadata": {
        "id": "BaIiAP3-YXxk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLYOCXDCvsx9",
        "outputId": "15f288db-0a65-4667-dafe-90b29e880e65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw Graph:\n",
            "[Log(Variable('x' 0.458, g=0), g=0), Sigmoid(Log(Variable('x' 0.458, g=0), g=0), g=0)]\n",
            "\n",
            "Topological Graph:\n",
            "Variable('x' 0.458, g=0)\n",
            "Log(Variable('x' 0.458, g=0), g=0)\n",
            "Sigmoid(Log(Variable('x' 0.458, g=0), g=0), g=0)\n",
            "\n",
            "Forward value:\n",
            "0.829333216455861\n",
            "\n",
            "\n",
            "Backward graph:\n",
            "[Log(Variable('x' 0.458, g=0.22376127937065252), g=0.141539632538837), Sigmoid(Log(Variable('x' 0.458, g=0.22376127937065252), g=0.141539632538837), g=1)]\n",
            "\n",
            "0.6712838381119576\n"
          ]
        }
      ],
      "source": [
        "with Graph() as graph:\n",
        "    x = Variable(0.458, \"x\")\n",
        "    y = Sigmoid(Exp(x))\n",
        "    \n",
        "print(\"Raw Graph:\")\n",
        "print(graph.values)\n",
        "print()\n",
        "\n",
        "print(\"Topological Graph:\")\n",
        "topo = generate_topo(y)\n",
        "for item in topo:\n",
        "    print(item)\n",
        "print()\n",
        "\n",
        "print(\"Forward value:\")\n",
        "print(graph.forward())\n",
        "print()\n",
        "print()\n",
        "\n",
        "print(\"Backward graph:\")\n",
        "graph.backward()\n",
        "print(graph.values)\n",
        "print()\n",
        "\n",
        "grad_graph = create_gradient_graph(graph.backward())\n",
        "print(grad_graph.wrt(x))\n",
        "\n",
        "del graph"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Network:\n",
        "    def __init__(self, lrate = 1e-2):\n",
        "        self.attrs = dir(self)\n",
        "        self.lrate = lrate\n",
        "        self.layers = []\n",
        "\n",
        "    def register(self):\n",
        "        for attr in dir(self):\n",
        "            if attr not in self.attrs:\n",
        "                if isinstance(getattr(self, attr), NetworkOperation):\n",
        "                    self.layers.append(getattr(self, attr))\n",
        "\n",
        "    def backward(self, grads):\n",
        "        for layer in self.layers:\n",
        "            layer.backward(grads, self.lrate)\n",
        "\n",
        "class NetworkOperation(ABC):\n",
        "    def __init__(self, *vars):\n",
        "        self.vars = vars"
      ],
      "metadata": {
        "id": "SNRlzxfnYo8x"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Linear(NetworkOperation):\n",
        "    def __init__(self, input_features, output_features, has_bias = True):\n",
        "        self.input_features = input_features\n",
        "        self.output_features = output_features\n",
        "        self.weights = np_vectorize(np.random.random((input_features, output_features)))\n",
        "        self.bias = np_vectorize(np.random.random(output_features)) if has_bias else np_vectorize(np.zeros(output_features))\n",
        "        super().__init__(self.weights)\n",
        "\n",
        "    def backward(self, grad, lrate):\n",
        "        grads = grad.wrt(self.weights)\n",
        "        for i, weight in np.ndenumerate(self.weights):\n",
        "            weight.value -= lrate * list(grads)[sum(i)].forward()\n",
        "\n",
        "        grads = grad.wrt(self.bias)\n",
        "        for i, bias in np.ndenumerate(self.bias):\n",
        "            bias.value -= lrate * list(grads)[sum(i)].forward()\n",
        "    \n",
        "    def __call__(self, x):\n",
        "        return np.add(np.dot(x, self.weights), self.bias)"
      ],
      "metadata": {
        "id": "Wka4v60CZS5x"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(Network):\n",
        "    def __init__(self, input_size, mid_size, output_size):\n",
        "        super().__init__()\n",
        "        self.inp = Linear(input_size, mid_size)\n",
        "        self.middle = Linear(mid_size, mid_size)\n",
        "        self.output = Linear(mid_size, output_size)\n",
        "        super().register()\n",
        "\n",
        "    def forward(self, x):\n",
        "        inp = self.inp(x)\n",
        "        mid = self.middle(inp)\n",
        "        return self.output(mid)"
      ],
      "metadata": {
        "id": "UOaLvZY3ZEZ6"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import tqdm\n",
        "np.random.seed(0)\n",
        "\n",
        "input_size = 5\n",
        "mid_size = 6\n",
        "output_size = 10\n",
        "\n",
        "x = np_vectorize(np.random.random(input_size))\n",
        "y_true = np_vectorize(np.random.random(output_size))\n",
        "\n",
        "mod = Model(input_size, mid_size, output_size)\n",
        "criterion = MSELoss()\n",
        "\n",
        "\n",
        "losses = []\n",
        "with Graph() as graph:\n",
        "    for i in tqdm.tqdm(range(100)):\n",
        "        y_pred = mod.forward(x)\n",
        "        loss = criterion(y_true, y_pred)\n",
        "        losses.append(loss.forward())\n",
        "\n",
        "        grad_graph = create_gradient_graph(loss.backward())\n",
        "        mod.backward(grad_graph)\n",
        "\n",
        "plt.plot(losses)\n",
        "plt.xlabel(\"Time step\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"3 linear layer learning\")\n",
        "plt.show()\n",
        "\n",
        "del graph\n",
        "\n",
        "print(\"Final loss: \"+str(losses[-1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "id": "ibTr_E1waWZs",
        "outputId": "ccf6a495-1eb2-4f92-af52-9da17e46be53"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:54<00:00,  1.82it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final loss: 0.25867663303270366\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import tqdm\n",
        "np.random.seed(0)\n",
        "\n",
        "input_size = 5\n",
        "output_size = 10\n",
        "lrate = 0.01\n",
        "\n",
        "def update_weights(weights, gradients, lrate):\n",
        "    for i, weight in np.ndenumerate(weights):\n",
        "        weight.value -= lrate * list(gradients)[sum(i)].forward()\n",
        "\n",
        "x = np_vectorize(np.random.random(input_size))\n",
        "y_true = np_vectorize(np.random.random(output_size))\n",
        "weights = np_vectorize(np.random.random((input_size, output_size)))\n",
        "weights2 = np_vectorize(np.random.random((output_size, output_size)))\n",
        "\n",
        "losses = []\n",
        "with Graph() as graph:\n",
        "    for i in tqdm.tqdm(range(10)):\n",
        "        m = np.dot(x, weights)\n",
        "        y_pred = np.dot(m, weights2)\n",
        "        loss = np.sum((y_true - y_pred) * (y_true - y_pred))\n",
        "        losses.append(loss.forward())\n",
        "\n",
        "        grad_graph = create_gradient_graph(loss.backward())\n",
        "        update_weights(weights, grad_graph.wrt(weights), lrate)\n",
        "        update_weights(weights2, grad_graph.wrt(weights2), lrate)\n",
        "\n",
        "plt.plot(losses)\n",
        "plt.xlabel(\"Time step\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Single linear layer learning\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "del graph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "2zmQ5rQVIPUP",
        "outputId": "8b329c6b-0093-420a-d03e-2ebd0b691263"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:05<00:00,  1.86it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}